{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfaff3c",
   "metadata": {},
   "source": [
    "# Set up the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a66b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac37585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/RareDis_Pipeline/Datasets_with_only_gold_entities/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in open(Path, 'r'):\n",
    "    train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ce62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]['doc_key'] = train[i].pop('doc')\n",
    "    train[i]['sentences'] = train[i].pop('tokens')\n",
    "    train[i]['ner'] = train[i].pop('entities')\n",
    "\n",
    "    train[i]['ner_modified'] = train[i]['ner'].copy()\n",
    "    train[i]['sentences_modified'] = train[i]['sentences'].copy()\n",
    "\n",
    "    train[i].pop('start')\n",
    "    train[i].pop('end')\n",
    "    train[i].pop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2intlist(s):\n",
    "    l = []\n",
    "    l.append(int(s.split(',')[0]))\n",
    "    l.append(int(s.split(',')[1]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5a76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f422e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner_modified\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner_modified'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner_modified'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner_modified'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner_modified'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c7971",
   "metadata": {},
   "source": [
    "# Remove discontinuous entities with more than 2 fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a5e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        if len(train[i]['ner_modified'][j]) >= 4:\n",
    "            indices_to_remove.add(j)\n",
    "            \n",
    "    new_list = [sublist for idx, sublist in enumerate(train[i]['ner_modified']) if idx not in indices_to_remove]\n",
    "    \n",
    "    train[i]['ner_modified'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a7ccc",
   "metadata": {},
   "source": [
    "# Check non-overlapped and overlapped discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of non-overlapped fragments for the current entity\n",
    "\n",
    "def num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx): \n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    if 0 < current_entitiy_idx < len(ner_lists) - 1: # Neither the 1st nor the last entity\n",
    "\n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1): # -1 excludes entity types such as 'SIGN'\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "      \n",
    "            if (previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "                and previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    elif current_entitiy_idx == 0: # 1st entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "            and cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    else: # Last entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos > previous_entity_last_fragment_right_pos\n",
    "            and cur_entity_cur_fragment_right_pos > previous_entity_last_fragment_right_pos):\n",
    "                num += 1\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "    if num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx) == len(ner_lists[current_entitiy_idx]) - 1:\n",
    "        return False # All fragments of current entity do not overlap with other entities\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d772e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5th discontinuous entity in doc_0 is overlapped.\n",
      "The 6th discontinuous entity in doc_0 is overlapped.\n",
      "The 19th discontinuous entity in doc_1 is overlapped.\n",
      "The 20th discontinuous entity in doc_1 is overlapped.\n",
      "The 7th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 11th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_7 is overlapped.\n",
      "The 7th discontinuous entity in doc_7 is overlapped.\n",
      "The 3th discontinuous entity in doc_11 is non-overlapped.\n",
      "The 10th discontinuous entity in doc_13 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_14 is overlapped.\n",
      "The 20th discontinuous entity in doc_14 is overlapped.\n",
      "The 21th discontinuous entity in doc_14 is overlapped.\n",
      "The 28th discontinuous entity in doc_14 is overlapped.\n",
      "The 2th discontinuous entity in doc_17 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_17 is non-overlapped.\n",
      "The 2th discontinuous entity in doc_18 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_19 is overlapped.\n",
      "The 11th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 14th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_19 is overlapped.\n",
      "The 16th discontinuous entity in doc_19 is overlapped.\n",
      "The 25th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_24 is overlapped.\n",
      "The 2th discontinuous entity in doc_25 is overlapped.\n",
      "The 4th discontinuous entity in doc_27 is overlapped.\n",
      "The 30th discontinuous entity in doc_29 is overlapped.\n",
      "The 31th discontinuous entity in doc_29 is overlapped.\n",
      "The 54th discontinuous entity in doc_29 is non-overlapped.\n",
      "The 5th discontinuous entity in doc_30 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_30 is overlapped.\n",
      "The 7th discontinuous entity in doc_30 is overlapped.\n",
      "The 8th discontinuous entity in doc_30 is overlapped.\n",
      "The 4th discontinuous entity in doc_32 is non-overlapped.\n",
      "The 17th discontinuous entity in doc_32 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_33 is overlapped.\n",
      "The 6th discontinuous entity in doc_33 is overlapped.\n",
      "The 9th discontinuous entity in doc_33 is overlapped.\n",
      "The 24th discontinuous entity in doc_34 is overlapped.\n",
      "The 36th discontinuous entity in doc_34 is overlapped.\n",
      "The 5th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 14th discontinuous entity in doc_36 is overlapped.\n",
      "The 15th discontinuous entity in doc_36 is overlapped.\n",
      "The 21th discontinuous entity in doc_36 is overlapped.\n",
      "The 24th discontinuous entity in doc_36 is overlapped.\n",
      "The 25th discontinuous entity in doc_36 is overlapped.\n",
      "The 26th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 28th discontinuous entity in doc_36 is overlapped.\n",
      "The 29th discontinuous entity in doc_36 is overlapped.\n",
      "The 30th discontinuous entity in doc_36 is overlapped.\n",
      "The 9th discontinuous entity in doc_39 is non-overlapped.\n",
      "The 11th discontinuous entity in doc_39 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_44 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_44 is overlapped.\n",
      "The 3th discontinuous entity in doc_45 is overlapped.\n",
      "The 13th discontinuous entity in doc_52 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_56 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_59 is overlapped.\n",
      "The 14th discontinuous entity in doc_59 is non-overlapped.\n",
      "The 21th discontinuous entity in doc_59 is overlapped.\n",
      "The 14th discontinuous entity in doc_60 is overlapped.\n",
      "The 5th discontinuous entity in doc_61 is overlapped.\n",
      "The 25th discontinuous entity in doc_61 is overlapped.\n",
      "The 26th discontinuous entity in doc_61 is overlapped.\n",
      "The 27th discontinuous entity in doc_61 is overlapped.\n",
      "The 32th discontinuous entity in doc_61 is non-overlapped.\n",
      "The 1th discontinuous entity in doc_63 is non-overlapped.\n",
      "The 5th discontinuous entity in doc_63 is overlapped.\n",
      "The 5th discontinuous entity in doc_65 is overlapped.\n",
      "The 6th discontinuous entity in doc_65 is overlapped.\n",
      "The 8th discontinuous entity in doc_71 is non-overlapped.\n",
      "The 16th discontinuous entity in doc_74 is non-overlapped.\n",
      "The 0th discontinuous entity in doc_75 is overlapped.\n",
      "The 1th discontinuous entity in doc_75 is overlapped.\n",
      "The 8th discontinuous entity in doc_75 is non-overlapped.\n",
      "The 10th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 26th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_78 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_80 is overlapped.\n",
      "The 16th discontinuous entity in doc_80 is non-overlapped.\n",
      "The 18th discontinuous entity in doc_83 is overlapped.\n",
      "The 18th discontinuous entity in doc_90 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_92 is overlapped.\n",
      "The 4th discontinuous entity in doc_92 is overlapped.\n",
      "The 5th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 8th discontinuous entity in doc_97 is overlapped.\n",
      "The 11th discontinuous entity in doc_97 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_97 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_100 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_102 is non-overlapped.\n",
      "The 5th discontinuous entity in doc_103 is overlapped.\n",
      "The 8th discontinuous entity in doc_103 is overlapped.\n",
      "The 9th discontinuous entity in doc_103 is overlapped.\n",
      "The 3th discontinuous entity in doc_104 is overlapped.\n",
      "The 4th discontinuous entity in doc_104 is overlapped.\n",
      "The 5th discontinuous entity in doc_104 is overlapped.\n",
      "The 8th discontinuous entity in doc_104 is overlapped.\n",
      "The 4th discontinuous entity in doc_108 is overlapped.\n",
      "The 9th discontinuous entity in doc_108 is overlapped.\n",
      "The 10th discontinuous entity in doc_108 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_108 is non-overlapped.\n",
      "The 2th discontinuous entity in doc_110 is overlapped.\n",
      "The 2th discontinuous entity in doc_112 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_114 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_118 is overlapped.\n",
      "The 4th discontinuous entity in doc_118 is overlapped.\n",
      "The 5th discontinuous entity in doc_118 is overlapped.\n",
      "The 9th discontinuous entity in doc_125 is overlapped.\n",
      "The 10th discontinuous entity in doc_125 is overlapped.\n",
      "The 24th discontinuous entity in doc_126 is overlapped.\n",
      "The 2th discontinuous entity in doc_127 is overlapped.\n"
     ]
    }
   ],
   "source": [
    "num_non_overlapped = 0\n",
    "num_overlapped = 0\n",
    "\n",
    "for doc in range(len(train)):\n",
    "    \n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "    \n",
    "    for current_entitiy_idx in range(len(ner_lists)): \n",
    "        \n",
    "        if len(ner_lists[current_entitiy_idx]) == 3: # 2-fragment discontinuous entities\n",
    "\n",
    "            num_nonoverlap_fragments = num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx)\n",
    "\n",
    "            if not check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is non-overlapped.\")\n",
    "                num_non_overlapped += 1\n",
    "            else:\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is overlapped.\")\n",
    "                num_overlapped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4315bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_non_overlapped is 47 and num_overlapped is 70\n"
     ]
    }
   ],
   "source": [
    "print(f'num_non_overlapped is {num_non_overlapped} and num_overlapped is {num_overlapped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da3b0a",
   "metadata": {},
   "source": [
    "# Check whether two fragments are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0465c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(fragment1, fragment2):\n",
    "    \n",
    "    # Extract the left and right positions of each fragment\n",
    "    left1, right1 = fragment1\n",
    "    left2, right2 = fragment2\n",
    "\n",
    "    # Check for overlap\n",
    "    if right1 < left2 or right2 < left1:\n",
    "        return False  # Non-overlapping fragments\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07348755",
   "metadata": {},
   "source": [
    "# Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81658a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 2nd fragment to the right of the 1st fragment\n",
    "\n",
    "def modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (including) the last token of the 1st fragment\n",
    "    tokens_before_1st_fragment = tokens[:ner_lists[current_entitiy_idx][0][1] + 1]\n",
    "    # All tokens of the 2nd fragment\n",
    "    tokens_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:ner_lists[current_entitiy_idx][1][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][0][1] + 1:]\n",
    "    \n",
    "    new_tokens = tokens_before_1st_fragment + tokens_2nd_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b00c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_1(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 2nd fragment\n",
    "    len_2nd_fragment = ner_lists[current_entitiy_idx][1][1] - ner_lists[current_entitiy_idx][1][0] + 1\n",
    "\n",
    "    last_token_pos_1st_fragment = ner_lists[current_entitiy_idx][0][1]\n",
    "    \n",
    "    # Modify offsets for the current discontinuous entity \n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][0][0], \n",
    "                                       ner_lists[current_entitiy_idx][0][1] + len_2nd_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)): \n",
    "\n",
    "        if idx != current_entitiy_idx:\n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "                \n",
    "                if (ner_lists[idx][k][0] > last_token_pos_1st_fragment \n",
    "                and ner_lists[idx][k][1] > last_token_pos_1st_fragment):\n",
    "                    ner_lists[idx][k][0] += len_2nd_fragment\n",
    "                    ner_lists[idx][k][1] += len_2nd_fragment   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3d48",
   "metadata": {},
   "source": [
    "# Rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785a881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 1st fragment to the left of the 2nd fragment\n",
    "\n",
    "def modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (excluding) the 1st token of the 2nd fragment\n",
    "    tokens_before_2nd_fragment = tokens[:ner_lists[current_entitiy_idx][1][0]]\n",
    "    # All tokens of the 1st fragment\n",
    "    tokens_1st_fragment = tokens[ner_lists[current_entitiy_idx][0][0]:ner_lists[current_entitiy_idx][0][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:]\n",
    "    \n",
    "    new_tokens = tokens_before_2nd_fragment + tokens_1st_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc176f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_2(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 1st fragment\n",
    "    len_1st_fragment = ner_lists[current_entitiy_idx][0][1] - ner_lists[current_entitiy_idx][0][0] + 1\n",
    "\n",
    "    first_token_pos_2nd_fragment = ner_lists[current_entitiy_idx][1][0]\n",
    "\n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][1][0], \n",
    "                                       ner_lists[current_entitiy_idx][1][1] + len_1st_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)):         \n",
    "\n",
    "        if idx != current_entitiy_idx:    \n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "\n",
    "                if (ner_lists[idx][k][0] >= first_token_pos_2nd_fragment \n",
    "                and ner_lists[idx][k][1] >= first_token_pos_2nd_fragment):\n",
    "                    ner_lists[idx][k][0] += len_1st_fragment\n",
    "                    ner_lists[idx][k][1] += len_1st_fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152b00e",
   "metadata": {},
   "source": [
    "# Modify sentences with 2-fragment discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13baf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    tokens = train[doc]['sentences_modified'] \n",
    "    # Changing tokens will NOT change train[doc]['sentences_modified'] for *operations* below\n",
    "    ner_lists = train[doc]['ner_modified'] \n",
    "    # Changing ner_lists WILL change train[doc]['ner_modified'] for *operations* below\n",
    "\n",
    "    for current_entitiy_idx in range(len(ner_lists)):\n",
    "\n",
    "        if len(ner_lists[current_entitiy_idx]) == 3:  # 2-fragment discontinuous entities\n",
    "            \n",
    "            interval_1st = ner_lists[current_entitiy_idx][0]\n",
    "            interval_2nd = ner_lists[current_entitiy_idx][1]\n",
    "\n",
    "            switch = 0\n",
    "\n",
    "            for idx in range(len(ner_lists)):\n",
    "\n",
    "                if idx != current_entitiy_idx:\n",
    "\n",
    "                    if len(ner_lists[idx]) == 2: # Continuous entities\n",
    "                        \n",
    "                        interval_cont = ner_lists[idx][0] \n",
    "                        \n",
    "                        if check_overlap(interval_2nd, interval_cont):\n",
    "                            switch = 1\n",
    "                            break\n",
    "                        \n",
    "                        elif check_overlap(interval_1st, interval_cont):\n",
    "                            switch = 3\n",
    "                            break\n",
    "                               \n",
    "                        \n",
    "                    elif len(ner_lists[idx]) == 3: # 2-fragment discontinuous entities\n",
    "                        \n",
    "                        interval_discont_1st = ner_lists[idx][0]\n",
    "                        interval_discont_2nd = ner_lists[idx][1]\n",
    "                                              \n",
    "                        if (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is False):\n",
    "                            switch = 2\n",
    "                            break\n",
    "\n",
    "                        elif (check_overlap(interval_1st, interval_discont_1st) is True \n",
    "                        and check_overlap(interval_2nd, interval_discont_2nd) is False):\n",
    "                            switch = 4\n",
    "                            break                             \n",
    "                            \n",
    "                        elif (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is True\n",
    "                           and interval_1st[1] > interval_discont_1st[1]\n",
    "                           and interval_2nd[0] > interval_discont_1st[0]):\n",
    "                            switch = 5\n",
    "                            break\n",
    "                            \n",
    "                        \n",
    "            if switch == 1 or switch == 2 or switch == 5:\n",
    "\n",
    "                tokens = modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_1(ner_lists, current_entitiy_idx)\n",
    "                \n",
    "                \n",
    "            elif switch == 0 or switch == 3 or switch == 4:\n",
    "\n",
    "                tokens = modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_2(ner_lists, current_entitiy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39ffdb",
   "metadata": {},
   "source": [
    "# Sanity check that all entities are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    l_modified = []\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        l_modified.append(train[doc]['sentences_modified'][train[doc]['ner_modified'][i][0][0]:train[doc]['ner_modified'][i][0][1] + 1])\n",
    "\n",
    "    l_original = []\n",
    "    for i in range(len(train[doc]['ner'])):\n",
    "        if len(train[doc]['ner'][i]) == 2:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1])\n",
    "        elif len(train[doc]['ner'][i]) == 3:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1] + train[doc]['sentences'][train[doc]['ner'][i][1][0]:train[doc]['ner'][i][1][1]+1])\n",
    "        else:\n",
    "            pass        \n",
    "        \n",
    "    if l_modified != l_original:          \n",
    "        print(f'The doc_{doc} needs more examination.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132bef7",
   "metadata": {},
   "source": [
    "# Add relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3a42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/RareDis_Pipeline/Datasets_with_gold_relations/dev_re.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0a05ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_re = []\n",
    "for line in open(Path, 'r'):\n",
    "    train_re.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb26dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/RareDis_Pipeline/Step2_cache_double_labels/dev_cache.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7de88233",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []\n",
    "for line in open(Path, 'r'):\n",
    "    cache.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c9a0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "159fad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    if not train[doc]['doc_key'] == train_re[doc]['doc'] == cache[doc]['doc']:\n",
    "        print('Document key error happens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f790fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether all entities are continuous or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39764893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        if len(train[doc]['ner_modified'][i]) != 2:\n",
    "            print('There are still discontinuous entities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f07549bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72e2ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[15, 15], ['SIGN', 'DISEASE']], [[81, 82], ['SIGN', 'SYMPTOM', 'DISEASE']]]\n"
     ]
    }
   ],
   "source": [
    "def find_lists_same_offsets_diff_types(list_of_lists):\n",
    "    # Maintain a dictionary where keys are offsets and values are sets of types\n",
    "    offset_to_types = {}\n",
    "    for lst in list_of_lists:\n",
    "        offset, type = tuple(lst[0]), lst[1]\n",
    "        if offset not in offset_to_types:\n",
    "            offset_to_types[offset] = set()\n",
    "        offset_to_types[offset].add(type)\n",
    "\n",
    "    # Keep only those entries with more than one type\n",
    "    result = [[list(offset), list(types)] for offset, types in offset_to_types.items() if len(types) > 1]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "list_of_lists = [[[0, 1], 'RAREDISEASE'],\n",
    "                 [[5, 6], 'DISEASE'],\n",
    "                 [[15, 15], 'DISEASE'],\n",
    "                 [[15, 15], 'SIGN'],\n",
    "                 [[53, 53], 'SYMPTOM'],\n",
    "                 [[60, 60], 'DISEASE'],\n",
    "                 [[70, 77], 'SIGN'],\n",
    "                 [[79, 79], 'SYMPTOM'],\n",
    "                 [[81, 82], 'SIGN'],\n",
    "                 [[81, 82], 'DISEASE'],\n",
    "                 [[81, 82], 'SYMPTOM'],\n",
    "                 [[96, 97], 'SIGN'],\n",
    "                 [[99, 101], 'SIGN']]\n",
    "\n",
    "print(find_lists_same_offsets_diff_types(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b04ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "6\n",
      "7\n",
      "12\n",
      "13\n",
      "14\n",
      "19\n",
      "21\n",
      "23\n",
      "24\n",
      "25\n",
      "33\n",
      "34\n",
      "39\n",
      "44\n",
      "45\n",
      "47\n",
      "52\n",
      "53\n",
      "56\n",
      "60\n",
      "61\n",
      "74\n",
      "75\n",
      "77\n",
      "78\n",
      "81\n",
      "83\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "93\n",
      "94\n",
      "97\n",
      "102\n",
      "107\n",
      "114\n",
      "126\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "for doc in range(len(train)):\n",
    "    if find_lists_same_offsets_diff_types(train[doc]['ner_modified']) != []:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e85ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5777cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2372d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists):\n",
    "    \n",
    "    # All indices of discontinuous entities with more than two fragments    \n",
    "    ner_lists_raw_removed = []\n",
    "    for idx in range(len(ner_lists_raw)):\n",
    "        if len(ner_lists_raw[idx]) > 3:\n",
    "            ner_lists_raw_removed.append(idx)\n",
    "\n",
    "    # Create a dictionary to store the names and associated lists\n",
    "    dict_ner_lists_raw = {f\"T{i+1}\": ner_lists_raw[i] for i in range(len(ner_lists_raw))}\n",
    "\n",
    "    # Remove the elements in ner_lists_raw_removed from the dictionary\n",
    "    for idx in ner_lists_raw_removed:\n",
    "        del dict_ner_lists_raw[f\"T{idx+1}\"]\n",
    "\n",
    "    # Convert the dictionary items to a list\n",
    "    items = list(dict_ner_lists_raw.items())\n",
    "\n",
    "    # Create a new dictionary after deleting discontinuous entities with more than two fragments   \n",
    "    new_dict_ner_lists = {}\n",
    "    for k in range(len(ner_lists)):\n",
    "        new_dict_ner_lists[items[k][0]] = ner_lists[k]\n",
    "        \n",
    "    return new_dict_ner_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac398aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    train[doc]['relations'] = []\n",
    "    \n",
    "    ner_lists_raw = train[doc]['ner']\n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "        \n",
    "    new_dict_ner_lists = create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists)\n",
    "    \n",
    "    if cache[doc]['remove'] != []:\n",
    "\n",
    "        \n",
    "        for i in range(len(cache[doc]['remove'])):\n",
    "\n",
    "            a_num = cache[doc]['remove'][i][0][1]\n",
    "            a_name = cache[doc]['remove'][i][0][0]\n",
    "\n",
    "            b_num = cache[doc]['remove'][i][1][1]\n",
    "            b_name = cache[doc]['remove'][i][1][0]\n",
    "\n",
    "            if a_num == 0 and b_num == 0:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                train[doc]['ner_modified'].remove(new_dict_ner_lists[cache[doc]['remove'][i][chosen_element][0]])\n",
    "\n",
    "                if 'relations' in train_re[doc]:\n",
    "\n",
    "                    for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                        for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                            if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                                rel_temp = []\n",
    "                                rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                                rel_temp.append(key)\n",
    "\n",
    "                        train[doc]['relations'].append(rel_temp)\n",
    "\n",
    "\n",
    "            elif (a_num == 0 and b_num >= 1) or (b_num == 0 and a_num >= 1):\n",
    "\n",
    "                if a_num == 0:\n",
    "\n",
    "                    train[doc]['ner_modified'].remove(new_dict_ner_lists[a_name])\n",
    "\n",
    "                elif b_num == 0:\n",
    "\n",
    "                    train[doc]['ner_modified'].remove(new_dict_ner_lists[b_name])\n",
    "\n",
    "\n",
    "                if 'relations' in train_re[doc]:\n",
    "\n",
    "                    for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                        for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                            if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                                rel_temp = []\n",
    "                                rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                                rel_temp.append(key)\n",
    "                            else:\n",
    "                                pass \n",
    "\n",
    "                        train[doc]['relations'].append(rel_temp)\n",
    "\n",
    "\n",
    "            elif a_num == 1 and b_num == 1:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                train[doc]['ner_modified'].remove(new_dict_ner_lists[cache[doc]['remove'][i][chosen_element][0]])\n",
    "\n",
    "                if 'relations' in train_re[doc]:\n",
    "\n",
    "                    for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                        for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                            if (value[0] in new_dict_ner_lists\n",
    "                            and value[1] in new_dict_ner_lists\n",
    "                            and value[0] != cache[doc]['remove'][i][chosen_element][0] \n",
    "                            and value[1] != cache[doc]['remove'][i][chosen_element][0]):\n",
    "                                rel_temp = []\n",
    "                                rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                                rel_temp.append(key) \n",
    "\n",
    "                        train[doc]['relations'].append(rel_temp)\n",
    "\n",
    "            else: # (a_num = 1, b_num = N) or (a_num = N, b_num = 1)\n",
    "\n",
    "\n",
    "                if a_num == 1:\n",
    "\n",
    "                    train[doc]['ner_modified'].remove(new_dict_ner_lists[a_name])\n",
    "\n",
    "                    if 'relations' in train_re[doc]:\n",
    "\n",
    "                        for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                            for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                                if (value[0] in new_dict_ner_lists\n",
    "                                and value[1] in new_dict_ner_lists \n",
    "                                and value[0] != a_name \n",
    "                                and value[1] != a_name):\n",
    "                                    rel_temp = []\n",
    "                                    rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                                    rel_temp.append(key)\n",
    "\n",
    "                            train[doc]['relations'].append(rel_temp)\n",
    "\n",
    "                elif b_num == 1:\n",
    "\n",
    "                    train[doc]['ner_modified'].remove(new_dict_ner_lists[b_name])\n",
    "\n",
    "                    if 'relations' in train_re[doc]:\n",
    "\n",
    "                        for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                            for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                                if (value[0] in new_dict_ner_lists\n",
    "                                and value[1] in new_dict_ner_lists \n",
    "                                and value[0] != b_name \n",
    "                                and value[1] != b_name):\n",
    "                                    rel_temp = []\n",
    "                                    rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                                    rel_temp.append(key)\n",
    "\n",
    "                            train[doc]['relations'].append(rel_temp)\n",
    "                            \n",
    "                            \n",
    "    elif cache[doc]['remove'] == []:\n",
    "        \n",
    "        if 'relations' in train_re[doc]:\n",
    "        \n",
    "            for i in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "                for key, value in train_re[doc]['relations'][i].items():\n",
    "\n",
    "                    if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                        rel_temp = []\n",
    "                        rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                        rel_temp.append(key) \n",
    "\n",
    "                train[doc]['relations'].append(rel_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39e903e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bbce605",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    if find_lists_same_offsets_diff_types(train[doc]['ner_modified']) != []:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff1af4",
   "metadata": {},
   "source": [
    "# Save the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c92bfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev_final_final.json', \"w\") as f_out:\n",
    "    for line in train:\n",
    "        f_out.write(json.dumps(line))\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
