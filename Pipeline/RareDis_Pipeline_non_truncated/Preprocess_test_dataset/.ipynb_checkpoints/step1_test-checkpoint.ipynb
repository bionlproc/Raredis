{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfaff3c",
   "metadata": {},
   "source": [
    "# Set up the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a66b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac37585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/RareDis_Pipeline/Datasets_with_only_gold_entities/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "096c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in open(Path, 'r'):\n",
    "    train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c7ce62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]['doc_key'] = train[i].pop('doc')\n",
    "    train[i]['sentences'] = train[i].pop('tokens')\n",
    "    train[i]['ner'] = train[i].pop('entities')\n",
    "\n",
    "    train[i]['ner_modified'] = train[i]['ner'].copy()\n",
    "    train[i]['sentences_modified'] = train[i]['sentences'].copy()\n",
    "\n",
    "    train[i].pop('start')\n",
    "    train[i].pop('end')\n",
    "    train[i].pop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f35689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2intlist(s):\n",
    "    l = []\n",
    "    l.append(int(s.split(',')[0]))\n",
    "    l.append(int(s.split(',')[1]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f5a76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f422e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner_modified\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner_modified'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner_modified'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner_modified'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner_modified'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c7971",
   "metadata": {},
   "source": [
    "# Remove discontinuous entities with more than 2 fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5a5e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        if len(train[i]['ner_modified'][j]) >= 4:\n",
    "            indices_to_remove.add(j)\n",
    "            \n",
    "    new_list = [sublist for idx, sublist in enumerate(train[i]['ner_modified']) if idx not in indices_to_remove]\n",
    "    \n",
    "    train[i]['ner_modified'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a7ccc",
   "metadata": {},
   "source": [
    "# Check non-overlapped and overlapped discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d79e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of non-overlapped fragments for the current entity\n",
    "\n",
    "def num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx): \n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    if 0 < current_entitiy_idx < len(ner_lists) - 1: # Neither the 1st nor the last entity\n",
    "\n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1): # -1 excludes entity types such as 'SIGN'\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "      \n",
    "            if (previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "                and previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    elif current_entitiy_idx == 0: # 1st entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "            and cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    else: # Last entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos > previous_entity_last_fragment_right_pos\n",
    "            and cur_entity_cur_fragment_right_pos > previous_entity_last_fragment_right_pos):\n",
    "                num += 1\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4ec411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "    if num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx) == len(ner_lists[current_entitiy_idx]) - 1:\n",
    "        return False # All fragments of current entity do not overlap with other entities\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40d772e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7th discontinuous entity in doc_1 is overlapped.\n",
      "The 8th discontinuous entity in doc_1 is overlapped.\n",
      "The 9th discontinuous entity in doc_1 is overlapped.\n",
      "The 10th discontinuous entity in doc_1 is overlapped.\n",
      "The 11th discontinuous entity in doc_1 is overlapped.\n",
      "The 12th discontinuous entity in doc_1 is overlapped.\n",
      "The 13th discontinuous entity in doc_1 is overlapped.\n",
      "The 14th discontinuous entity in doc_1 is overlapped.\n",
      "The 15th discontinuous entity in doc_1 is overlapped.\n",
      "The 16th discontinuous entity in doc_1 is overlapped.\n",
      "The 3th discontinuous entity in doc_5 is overlapped.\n",
      "The 4th discontinuous entity in doc_5 is overlapped.\n",
      "The 5th discontinuous entity in doc_12 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_13 is overlapped.\n",
      "The 3th discontinuous entity in doc_17 is overlapped.\n",
      "The 4th discontinuous entity in doc_17 is overlapped.\n",
      "The 5th discontinuous entity in doc_17 is overlapped.\n",
      "The 6th discontinuous entity in doc_17 is overlapped.\n",
      "The 6th discontinuous entity in doc_26 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_26 is overlapped.\n",
      "The 8th discontinuous entity in doc_26 is overlapped.\n",
      "The 14th discontinuous entity in doc_30 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_32 is overlapped.\n",
      "The 8th discontinuous entity in doc_32 is overlapped.\n",
      "The 7th discontinuous entity in doc_41 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_42 is overlapped.\n",
      "The 5th discontinuous entity in doc_42 is overlapped.\n",
      "The 6th discontinuous entity in doc_42 is overlapped.\n",
      "The 7th discontinuous entity in doc_42 is overlapped.\n",
      "The 8th discontinuous entity in doc_42 is overlapped.\n",
      "The 9th discontinuous entity in doc_42 is overlapped.\n",
      "The 10th discontinuous entity in doc_42 is overlapped.\n",
      "The 11th discontinuous entity in doc_42 is overlapped.\n",
      "The 16th discontinuous entity in doc_42 is overlapped.\n",
      "The 17th discontinuous entity in doc_42 is overlapped.\n",
      "The 26th discontinuous entity in doc_42 is overlapped.\n",
      "The 35th discontinuous entity in doc_42 is overlapped.\n",
      "The 2th discontinuous entity in doc_49 is overlapped.\n",
      "The 3th discontinuous entity in doc_49 is overlapped.\n",
      "The 6th discontinuous entity in doc_49 is overlapped.\n",
      "The 3th discontinuous entity in doc_54 is overlapped.\n",
      "The 5th discontinuous entity in doc_55 is overlapped.\n",
      "The 8th discontinuous entity in doc_55 is overlapped.\n",
      "The 6th discontinuous entity in doc_58 is overlapped.\n",
      "The 7th discontinuous entity in doc_58 is overlapped.\n",
      "The 16th discontinuous entity in doc_58 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_67 is overlapped.\n",
      "The 26th discontinuous entity in doc_69 is overlapped.\n",
      "The 13th discontinuous entity in doc_71 is non-overlapped.\n",
      "The 23th discontinuous entity in doc_75 is overlapped.\n",
      "The 24th discontinuous entity in doc_75 is overlapped.\n",
      "The 5th discontinuous entity in doc_78 is non-overlapped.\n",
      "The 2th discontinuous entity in doc_82 is overlapped.\n",
      "The 3th discontinuous entity in doc_82 is overlapped.\n",
      "The 4th discontinuous entity in doc_82 is overlapped.\n",
      "The 5th discontinuous entity in doc_82 is overlapped.\n",
      "The 6th discontinuous entity in doc_82 is overlapped.\n",
      "The 7th discontinuous entity in doc_82 is overlapped.\n",
      "The 8th discontinuous entity in doc_82 is overlapped.\n",
      "The 9th discontinuous entity in doc_82 is overlapped.\n",
      "The 10th discontinuous entity in doc_82 is overlapped.\n",
      "The 1th discontinuous entity in doc_88 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_89 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_93 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_94 is overlapped.\n",
      "The 27th discontinuous entity in doc_94 is overlapped.\n",
      "The 31th discontinuous entity in doc_94 is overlapped.\n",
      "The 32th discontinuous entity in doc_94 is overlapped.\n",
      "The 35th discontinuous entity in doc_94 is overlapped.\n",
      "The 7th discontinuous entity in doc_96 is overlapped.\n",
      "The 8th discontinuous entity in doc_96 is overlapped.\n",
      "The 9th discontinuous entity in doc_96 is overlapped.\n",
      "The 11th discontinuous entity in doc_96 is overlapped.\n",
      "The 16th discontinuous entity in doc_96 is overlapped.\n",
      "The 35th discontinuous entity in doc_96 is overlapped.\n",
      "The 36th discontinuous entity in doc_96 is overlapped.\n",
      "The 2th discontinuous entity in doc_99 is overlapped.\n",
      "The 3th discontinuous entity in doc_99 is overlapped.\n",
      "The 3th discontinuous entity in doc_101 is overlapped.\n",
      "The 4th discontinuous entity in doc_101 is overlapped.\n"
     ]
    }
   ],
   "source": [
    "num_non_overlapped = 0\n",
    "num_overlapped = 0\n",
    "\n",
    "for doc in range(len(train)):\n",
    "    \n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "    \n",
    "    for current_entitiy_idx in range(len(ner_lists)): \n",
    "        \n",
    "        if len(ner_lists[current_entitiy_idx]) == 3: # 2-fragment discontinuous entities\n",
    "\n",
    "            num_nonoverlap_fragments = num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx)\n",
    "\n",
    "            if not check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is non-overlapped.\")\n",
    "                num_non_overlapped += 1\n",
    "            else:\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is overlapped.\")\n",
    "                num_overlapped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4315bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_non_overlapped is 10 and num_overlapped is 70\n"
     ]
    }
   ],
   "source": [
    "print(f'num_non_overlapped is {num_non_overlapped} and num_overlapped is {num_overlapped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da3b0a",
   "metadata": {},
   "source": [
    "# Check whether two fragments are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0465c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(fragment1, fragment2):\n",
    "    \n",
    "    # Extract the left and right positions of each fragment\n",
    "    left1, right1 = fragment1\n",
    "    left2, right2 = fragment2\n",
    "\n",
    "    # Check for overlap\n",
    "    if right1 < left2 or right2 < left1:\n",
    "        return False  # Non-overlapping fragments\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07348755",
   "metadata": {},
   "source": [
    "# Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b81658a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 2nd fragment to the right of the 1st fragment\n",
    "\n",
    "def modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (including) the last token of the 1st fragment\n",
    "    tokens_before_1st_fragment = tokens[:ner_lists[current_entitiy_idx][0][1] + 1]\n",
    "    # All tokens of the 2nd fragment\n",
    "    tokens_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:ner_lists[current_entitiy_idx][1][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][0][1] + 1:]\n",
    "    \n",
    "    new_tokens = tokens_before_1st_fragment + tokens_2nd_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b00c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_1(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 2nd fragment\n",
    "    len_2nd_fragment = ner_lists[current_entitiy_idx][1][1] - ner_lists[current_entitiy_idx][1][0] + 1\n",
    "\n",
    "    last_token_pos_1st_fragment = ner_lists[current_entitiy_idx][0][1]\n",
    "    \n",
    "    # Modify offsets for the current discontinuous entity \n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][0][0], \n",
    "                                       ner_lists[current_entitiy_idx][0][1] + len_2nd_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)): \n",
    "\n",
    "        if idx != current_entitiy_idx:\n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "                \n",
    "                if (ner_lists[idx][k][0] > last_token_pos_1st_fragment \n",
    "                and ner_lists[idx][k][1] > last_token_pos_1st_fragment):\n",
    "                    ner_lists[idx][k][0] += len_2nd_fragment\n",
    "                    ner_lists[idx][k][1] += len_2nd_fragment   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3d48",
   "metadata": {},
   "source": [
    "# Rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "785a881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 1st fragment to the left of the 2nd fragment\n",
    "\n",
    "def modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (excluding) the 1st token of the 2nd fragment\n",
    "    tokens_before_2nd_fragment = tokens[:ner_lists[current_entitiy_idx][1][0]]\n",
    "    # All tokens of the 1st fragment\n",
    "    tokens_1st_fragment = tokens[ner_lists[current_entitiy_idx][0][0]:ner_lists[current_entitiy_idx][0][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:]\n",
    "    \n",
    "    new_tokens = tokens_before_2nd_fragment + tokens_1st_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc176f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_2(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 1st fragment\n",
    "    len_1st_fragment = ner_lists[current_entitiy_idx][0][1] - ner_lists[current_entitiy_idx][0][0] + 1\n",
    "\n",
    "    first_token_pos_2nd_fragment = ner_lists[current_entitiy_idx][1][0]\n",
    "\n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][1][0], \n",
    "                                       ner_lists[current_entitiy_idx][1][1] + len_1st_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)):         \n",
    "\n",
    "        if idx != current_entitiy_idx:    \n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "\n",
    "                if (ner_lists[idx][k][0] >= first_token_pos_2nd_fragment \n",
    "                and ner_lists[idx][k][1] >= first_token_pos_2nd_fragment):\n",
    "                    ner_lists[idx][k][0] += len_1st_fragment\n",
    "                    ner_lists[idx][k][1] += len_1st_fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152b00e",
   "metadata": {},
   "source": [
    "# Modify sentences with 2-fragment discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "13baf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    tokens = train[doc]['sentences_modified'] \n",
    "    # Changing tokens will NOT change train[doc]['sentences_modified'] for *operations* below\n",
    "    ner_lists = train[doc]['ner_modified'] \n",
    "    # Changing ner_lists WILL change train[doc]['ner_modified'] for *operations* below\n",
    "\n",
    "    for current_entitiy_idx in range(len(ner_lists)):\n",
    "\n",
    "        if len(ner_lists[current_entitiy_idx]) == 3:  # 2-fragment discontinuous entities\n",
    "            \n",
    "            interval_1st = ner_lists[current_entitiy_idx][0]\n",
    "            interval_2nd = ner_lists[current_entitiy_idx][1]\n",
    "\n",
    "            switch = 0\n",
    "\n",
    "            for idx in range(len(ner_lists)):\n",
    "\n",
    "                if idx != current_entitiy_idx:\n",
    "\n",
    "                    if len(ner_lists[idx]) == 2: # Continuous entities\n",
    "                        \n",
    "                        interval_cont = ner_lists[idx][0] \n",
    "                        \n",
    "                        if check_overlap(interval_2nd, interval_cont):\n",
    "                            switch = 1\n",
    "                            break\n",
    "                        \n",
    "                        elif check_overlap(interval_1st, interval_cont):\n",
    "                            switch = 3\n",
    "                            break\n",
    "                               \n",
    "                        \n",
    "                    elif len(ner_lists[idx]) == 3: # 2-fragment discontinuous entities\n",
    "                        \n",
    "                        interval_discont_1st = ner_lists[idx][0]\n",
    "                        interval_discont_2nd = ner_lists[idx][1]\n",
    "                                              \n",
    "                        if (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is False):\n",
    "                            switch = 2\n",
    "                            break\n",
    "\n",
    "                        elif (check_overlap(interval_1st, interval_discont_1st) is True \n",
    "                        and check_overlap(interval_2nd, interval_discont_2nd) is False):\n",
    "                            switch = 4\n",
    "                            break                             \n",
    "                            \n",
    "                        elif (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is True\n",
    "                           and interval_1st[1] > interval_discont_1st[1]\n",
    "                           and interval_2nd[0] > interval_discont_1st[0]):\n",
    "                            switch = 5\n",
    "                            break\n",
    "                            \n",
    "                        \n",
    "            if switch == 1 or switch == 2 or switch == 5:\n",
    "\n",
    "                tokens = modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_1(ner_lists, current_entitiy_idx)\n",
    "                \n",
    "                \n",
    "            elif switch == 0 or switch == 3 or switch == 4:\n",
    "\n",
    "                tokens = modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_2(ner_lists, current_entitiy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39ffdb",
   "metadata": {},
   "source": [
    "# Sanity check that all entities are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a9bef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doc_1 needs more examination.\n",
      "The doc_5 needs more examination.\n",
      "The doc_42 needs more examination.\n",
      "The doc_82 needs more examination.\n",
      "The doc_96 needs more examination.\n"
     ]
    }
   ],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    l_modified = []\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        l_modified.append(train[doc]['sentences_modified'][train[doc]['ner_modified'][i][0][0]:train[doc]['ner_modified'][i][0][1] + 1])\n",
    "\n",
    "    l_original = []\n",
    "    for i in range(len(train[doc]['ner'])):\n",
    "        if len(train[doc]['ner'][i]) == 2:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1])\n",
    "        elif len(train[doc]['ner'][i]) == 3:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1] + train[doc]['sentences'][train[doc]['ner'][i][1][0]:train[doc]['ner'][i][1][1]+1])\n",
    "        else:\n",
    "            pass        \n",
    "        \n",
    "    if l_modified != l_original:          \n",
    "        print(f'The doc_{doc} needs more examination.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fbb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "64a16906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discoloration', 'of', 'the', 'discolored']\n",
      "['over', 'part', 'of', 'the', 'outer', 'ear', 'discolored']\n",
      "['the', 'sclera']\n"
     ]
    }
   ],
   "source": [
    "doc = 1\n",
    "\n",
    "l_modified = []\n",
    "for i in range(len(train[doc]['ner_modified'])):\n",
    "    l_modified.append(train[doc]['sentences_modified'][train[doc]['ner_modified'][i][0][0]:train[doc]['ner_modified'][i][0][1] + 1])\n",
    "\n",
    "l_original = []\n",
    "for i in range(len(train[doc]['ner'])):\n",
    "    if len(train[doc]['ner'][i]) == 2:\n",
    "        l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1])\n",
    "    elif len(train[doc]['ner'][i]) == 3:\n",
    "        l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1] + train[doc]['sentences'][train[doc]['ner'][i][1][0]:train[doc]['ner'][i][1][1]+1])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(len(l_modified)):\n",
    "    if l_modified[i] != l_original[i]:\n",
    "        print(l_original[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679120a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e184f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i].pop('sentences')\n",
    "    train[i].pop('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2f889eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i]['sentences'] = train[i].pop('sentences_modified')\n",
    "    train[i]['ner'] = train[i].pop('ner_modified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "895790ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 'RAREDISEASE']\n"
     ]
    }
   ],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flattened_list = []\n",
    "    for sublist in nested_list:\n",
    "        if isinstance(sublist, list):\n",
    "            flattened_list.extend(sublist)\n",
    "        else:\n",
    "            flattened_list.append(sublist)\n",
    "    return flattened_list\n",
    "\n",
    "nested_list = [[0, 1], 'RAREDISEASE']\n",
    "print(flatten_list(nested_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "23824b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i]['ner'])):\n",
    "        train[i]['ner'][j] = flatten_list(train[i]['ner'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9008b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b5e71a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]['relations'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f7c86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_key': 'Pediatric-Crohns-Disease_test2_-1_-1', 'sentences': ['Crohn', \"'\", 's', 'disease', 'is', 'a', 'common', 'disorder', ',', 'affecting', 'as', 'many', 'as', '780', ',', '000', 'people', 'in', 'the', 'United', 'States', '.', 'The', 'disorder', 'is', 'most', 'common', 'in', 'individuals', 'between', '15', '-', '35', ',', 'with', 'approximately', '25', '%', 'diagnosed', 'by', 'age', '20', '.', 'There', 'is', 'another', 'increase', 'in', 'frequency', 'among', 'individuals', 'between', '60', '-', '80', 'years', 'of', 'age', '.', 'The', 'frequency', 'of', 'pediatric', 'Crohn', \"'\", 's', 'disease', 'is', 'increasing', ';', 'in', 'particular', ',', 'there', 'has', 'been', 'a', 'recent', 'increase', 'in', 'the', 'incidence', 'of', 'Crohn', \"'\", 's', 'disease', 'in', 'children', 'less', 'than', '6', 'years', 'old', ';', 'this', 'is', 'called', 'very', 'early', 'onset', '(', 'VEO', ')', 'inflammatory', 'bowel', 'disease', '.', 'Although', 'more', 'women', 'are', 'affected', 'by', 'Crohn', \"'\", 's', 'disease', 'than', 'men', ',', 'pediatric', 'Crohn', \"'\", 's', 'disease', 'is', 'more', 'common', 'in', 'boys', 'than', 'girls', '.', 'Pediatric', 'Crohn', \"'\", 's', 'disease', 'is', 'more', 'common', 'in', 'Caucasians', 'than', 'in', 'people', 'of', 'African', 'descent', '.', 'It', 'is', 'rare', 'in', 'people', 'of', 'Asian', 'and', 'Hispanic', 'descent', '.', 'Generally', ',', 'Crohn', \"'\", 's', 'disease', 'is', 'more', 'severe', 'among', 'children', 'and', 'adolescents', 'than', 'in', 'adults', '.'], 'ner': [[0, 3, 'RAREDISEASE'], [22, 23, 'ANAPHOR'], [63, 66, 'RAREDISEASE'], [83, 86, 'RAREDISEASE'], [104, 106, 'RAREDISEASE'], [114, 117, 'RAREDISEASE'], [121, 125, 'RAREDISEASE'], [134, 138, 'RAREDISEASE'], [135, 138, 'RAREDISEASE'], [151, 151, 'ANAPHOR'], [164, 167, 'RAREDISEASE']], 'relations': []}\n"
     ]
    }
   ],
   "source": [
    "print(train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff1af4",
   "metadata": {},
   "source": [
    "# Save the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c92bfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', \"w\") as f_out:\n",
    "    for line in train:\n",
    "        f_out.write(json.dumps(line))\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
