{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfaff3c",
   "metadata": {},
   "source": [
    "# Set up the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a66b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac37585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/Raredis/Pipeline/raw_data/train/train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "096c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in open(Path, 'r'):\n",
    "    train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7ce62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]['doc_key'] = train[i].pop('doc')\n",
    "    train[i]['sentences'] = train[i].pop('tokens')\n",
    "    train[i]['ner'] = train[i].pop('entities')\n",
    "\n",
    "    train[i]['ner_modified'] = train[i]['ner'].copy()\n",
    "    train[i]['sentences_modified'] = train[i]['sentences'].copy()\n",
    "\n",
    "    train[i].pop('start')\n",
    "    train[i].pop('end')\n",
    "    train[i].pop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f35689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2intlist(s):\n",
    "    l = []\n",
    "    l.append(int(s.split(',')[0]))\n",
    "    l.append(int(s.split(',')[1]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f5a76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f422e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner_modified\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner_modified'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner_modified'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner_modified'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner_modified'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c7971",
   "metadata": {},
   "source": [
    "# Remove discontinuous entities with more than 2 fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5a5e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        if len(train[i]['ner_modified'][j]) >= 4:\n",
    "            indices_to_remove.add(j)\n",
    "            \n",
    "    new_list = [sublist for idx, sublist in enumerate(train[i]['ner_modified']) if idx not in indices_to_remove]\n",
    "    \n",
    "    train[i]['ner_modified'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a7ccc",
   "metadata": {},
   "source": [
    "# Check non-overlapped and overlapped discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d79e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of non-overlapped fragments for the current entity\n",
    "\n",
    "def num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx): \n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    if 0 < current_entitiy_idx < len(ner_lists) - 1: # Neither the 1st nor the last entity\n",
    "\n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1): # -1 excludes entity types such as 'SIGN'\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "      \n",
    "            if (previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "                and previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    elif current_entitiy_idx == 0: # 1st entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "            and cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    else: # Last entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos > previous_entity_last_fragment_right_pos\n",
    "            and cur_entity_cur_fragment_right_pos > previous_entity_last_fragment_right_pos):\n",
    "                num += 1\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ec411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "    if num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx) == len(ner_lists[current_entitiy_idx]) - 1:\n",
    "        return False # All fragments of current entity do not overlap with other entities\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40d772e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_overlapped = 0\n",
    "num_overlapped = 0\n",
    "\n",
    "for doc in range(len(train)):\n",
    "    \n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "    \n",
    "    for current_entitiy_idx in range(len(ner_lists)): \n",
    "        \n",
    "        if len(ner_lists[current_entitiy_idx]) == 3: # 2-fragment discontinuous entities\n",
    "\n",
    "            num_nonoverlap_fragments = num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx)\n",
    "\n",
    "            if not check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "                # print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is non-overlapped.\")\n",
    "                num_non_overlapped += 1\n",
    "            else:\n",
    "                # print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is overlapped.\")\n",
    "                num_overlapped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4315bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_non_overlapped is 173 and num_overlapped is 310\n"
     ]
    }
   ],
   "source": [
    "print(f'num_non_overlapped is {num_non_overlapped} and num_overlapped is {num_overlapped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da3b0a",
   "metadata": {},
   "source": [
    "# Check whether two fragments are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0465c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(fragment1, fragment2):\n",
    "    \n",
    "    # Extract the left and right positions of each fragment\n",
    "    left1, right1 = fragment1\n",
    "    left2, right2 = fragment2\n",
    "\n",
    "    # Check for overlap\n",
    "    if right1 < left2 or right2 < left1:\n",
    "        return False  # Non-overlapping fragments\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07348755",
   "metadata": {},
   "source": [
    "# Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b81658a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 2nd fragment to the right of the 1st fragment\n",
    "\n",
    "def modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (including) the last token of the 1st fragment\n",
    "    tokens_before_1st_fragment = tokens[:ner_lists[current_entitiy_idx][0][1] + 1]\n",
    "    # All tokens of the 2nd fragment\n",
    "    tokens_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:ner_lists[current_entitiy_idx][1][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][0][1] + 1:]\n",
    "    \n",
    "    new_tokens = tokens_before_1st_fragment + tokens_2nd_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b00c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_1(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 2nd fragment\n",
    "    len_2nd_fragment = ner_lists[current_entitiy_idx][1][1] - ner_lists[current_entitiy_idx][1][0] + 1\n",
    "\n",
    "    last_token_pos_1st_fragment = ner_lists[current_entitiy_idx][0][1]\n",
    "    \n",
    "    # Modify offsets for the current discontinuous entity \n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][0][0], \n",
    "                                       ner_lists[current_entitiy_idx][0][1] + len_2nd_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)): \n",
    "\n",
    "        if idx != current_entitiy_idx:\n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "                \n",
    "                if (ner_lists[idx][k][0] > last_token_pos_1st_fragment \n",
    "                and ner_lists[idx][k][1] > last_token_pos_1st_fragment):\n",
    "                    ner_lists[idx][k][0] += len_2nd_fragment\n",
    "                    ner_lists[idx][k][1] += len_2nd_fragment   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3d48",
   "metadata": {},
   "source": [
    "# Rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "785a881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 1st fragment to the left of the 2nd fragment\n",
    "\n",
    "def modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (excluding) the 1st token of the 2nd fragment\n",
    "    tokens_before_2nd_fragment = tokens[:ner_lists[current_entitiy_idx][1][0]]\n",
    "    # All tokens of the 1st fragment\n",
    "    tokens_1st_fragment = tokens[ner_lists[current_entitiy_idx][0][0]:ner_lists[current_entitiy_idx][0][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:]\n",
    "    \n",
    "    new_tokens = tokens_before_2nd_fragment + tokens_1st_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc176f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_2(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 1st fragment\n",
    "    len_1st_fragment = ner_lists[current_entitiy_idx][0][1] - ner_lists[current_entitiy_idx][0][0] + 1\n",
    "\n",
    "    first_token_pos_2nd_fragment = ner_lists[current_entitiy_idx][1][0]\n",
    "\n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][1][0], \n",
    "                                       ner_lists[current_entitiy_idx][1][1] + len_1st_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)):         \n",
    "\n",
    "        if idx != current_entitiy_idx:    \n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "\n",
    "                if (ner_lists[idx][k][0] >= first_token_pos_2nd_fragment \n",
    "                and ner_lists[idx][k][1] >= first_token_pos_2nd_fragment):\n",
    "                    ner_lists[idx][k][0] += len_1st_fragment\n",
    "                    ner_lists[idx][k][1] += len_1st_fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152b00e",
   "metadata": {},
   "source": [
    "# Modify sentences with 2-fragment discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13baf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    tokens = train[doc]['sentences_modified'] \n",
    "    # Changing tokens will NOT change train[doc]['sentences_modified'] for *operations* below\n",
    "    ner_lists = train[doc]['ner_modified'] \n",
    "    # Changing ner_lists WILL change train[doc]['ner_modified'] for *operations* below\n",
    "\n",
    "    for current_entitiy_idx in range(len(ner_lists)):\n",
    "\n",
    "        if len(ner_lists[current_entitiy_idx]) == 3:  # 2-fragment discontinuous entities\n",
    "            \n",
    "            interval_1st = ner_lists[current_entitiy_idx][0]\n",
    "            interval_2nd = ner_lists[current_entitiy_idx][1]\n",
    "\n",
    "            switch = 0\n",
    "\n",
    "            for idx in range(len(ner_lists)):\n",
    "\n",
    "                if idx != current_entitiy_idx:\n",
    "\n",
    "                    if len(ner_lists[idx]) == 2: # Continuous entities\n",
    "                        \n",
    "                        interval_cont = ner_lists[idx][0] \n",
    "                        \n",
    "                        if check_overlap(interval_2nd, interval_cont):\n",
    "                            switch = 1\n",
    "                            break\n",
    "                        \n",
    "                        elif check_overlap(interval_1st, interval_cont):\n",
    "                            switch = 3\n",
    "                            break\n",
    "                               \n",
    "                        \n",
    "                    elif len(ner_lists[idx]) == 3: # 2-fragment discontinuous entities\n",
    "                        \n",
    "                        interval_discont_1st = ner_lists[idx][0]\n",
    "                        interval_discont_2nd = ner_lists[idx][1]\n",
    "                                              \n",
    "                        if (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is False):\n",
    "                            switch = 2\n",
    "                            break\n",
    "\n",
    "                        elif (check_overlap(interval_1st, interval_discont_1st) is True \n",
    "                        and check_overlap(interval_2nd, interval_discont_2nd) is False):\n",
    "                            switch = 4\n",
    "                            break                             \n",
    "                            \n",
    "                        elif (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is True\n",
    "                           and interval_1st[1] > interval_discont_1st[1]\n",
    "                           and interval_2nd[0] > interval_discont_1st[0]):\n",
    "                            switch = 5\n",
    "                            break\n",
    "                            \n",
    "                        \n",
    "            if switch == 1 or switch == 2 or switch == 5:\n",
    "\n",
    "                tokens = modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_1(ner_lists, current_entitiy_idx)\n",
    "                \n",
    "                \n",
    "            elif switch == 0 or switch == 3 or switch == 4:\n",
    "\n",
    "                tokens = modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_2(ner_lists, current_entitiy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39ffdb",
   "metadata": {},
   "source": [
    "# Sanity check that all entities are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a9bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    l_modified = []\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        l_modified.append(train[doc]['sentences_modified'][train[doc]['ner_modified'][i][0][0]:train[doc]['ner_modified'][i][0][1] + 1])\n",
    "\n",
    "    l_original = []\n",
    "    for i in range(len(train[doc]['ner'])):\n",
    "        if len(train[doc]['ner'][i]) == 2:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1])\n",
    "        elif len(train[doc]['ner'][i]) == 3:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1] + train[doc]['sentences'][train[doc]['ner'][i][1][0]:train[doc]['ner'][i][1][1]+1])\n",
    "        else:\n",
    "            pass        \n",
    "        \n",
    "    if l_modified != l_original:          \n",
    "        print(f'The doc_{doc} needs more examination.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ed29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a132bef7",
   "metadata": {},
   "source": [
    "# Add relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca3a42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/Raredis/Pipeline/raw_data/train/train_re.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0a05ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_re = []\n",
    "for line in open(Path, 'r'):\n",
    "    train_re.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3f6d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists):\n",
    "    \n",
    "    # All indices of discontinuous entities with more than two fragments    \n",
    "    ner_lists_raw_removed = []\n",
    "    for idx in range(len(ner_lists_raw)):\n",
    "        if len(ner_lists_raw[idx]) > 3:\n",
    "            ner_lists_raw_removed.append(idx)\n",
    "\n",
    "    # Create a dictionary to store the names and associated lists\n",
    "    dict_ner_lists_raw = {f\"T{i+1}\": ner_lists_raw[i] for i in range(len(ner_lists_raw))}\n",
    "\n",
    "    # Remove the elements in ner_lists_raw_removed from the dictionary\n",
    "    for idx in ner_lists_raw_removed:\n",
    "        del dict_ner_lists_raw[f\"T{idx+1}\"]\n",
    "\n",
    "    # Convert the dictionary items to a list\n",
    "    items = list(dict_ner_lists_raw.items())\n",
    "\n",
    "    # Create a new dictionary after deleting discontinuous entities with more than two fragments   \n",
    "    new_dict_ner_lists = {}\n",
    "    for k in range(len(ner_lists)):\n",
    "        new_dict_ner_lists[items[k][0]] = ner_lists[k]\n",
    "        \n",
    "    return new_dict_ner_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bd86e",
   "metadata": {},
   "source": [
    "# Cache entities with double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "332bba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for doc in range(len(train_re)):\n",
    "    \n",
    "    temp = {}\n",
    "    doc_name = train_re[doc]['doc']\n",
    "    temp['doc'] = doc_name\n",
    "    temp['remove'] = []\n",
    "    \n",
    "    # dictionary to store offsets and their keys\n",
    "    offset_keys = {}\n",
    "\n",
    "    # iterate over the list\n",
    "    for item in train_re[doc]['entities']:\n",
    "        for k, v in item.items():\n",
    "            \n",
    "            if len(v) == 2:\n",
    "            \n",
    "                offset = tuple(v[0])  # use tuple to make the list hashable\n",
    "                # store the key instead of the type\n",
    "                if offset not in offset_keys:\n",
    "                    offset_keys[offset] = [k]\n",
    "                else:\n",
    "                    offset_keys[offset].append(k)\n",
    "\n",
    "    # find and print the offsets with more than one key\n",
    "    for offset, keys in offset_keys.items():\n",
    "        \n",
    "        if len(keys) > 1:\n",
    "            \n",
    "            a = 0\n",
    "            b = 0\n",
    "\n",
    "            for l in train_re[doc]['relations']:\n",
    "\n",
    "                for k, v in l.items():\n",
    "                    if keys[0] in v:\n",
    "                        a += 1\n",
    "                    elif keys[1] in v:\n",
    "                        b += 1\n",
    "\n",
    "            # print(f'keys are {keys} and doc is {doc_name}')\n",
    "\n",
    "            # print(f'{keys[0]} has number {a}')\n",
    "            # print(f'{keys[1]} has number {b}')\n",
    "\n",
    "            # print('\\n')\n",
    "\n",
    "            l = []\n",
    "\n",
    "            l.append([keys[0], a])\n",
    "            l.append([keys[1], b])\n",
    "\n",
    "            temp['remove'].append(l)\n",
    "                \n",
    "                \n",
    "    res.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37642562",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60eb2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether all entities are continuous or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "359c79c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        if len(train[doc]['ner_modified'][i]) != 2:\n",
    "            print('There are still discontinuous entities.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778ed81",
   "metadata": {},
   "source": [
    "# Remove entities with double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a3a2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists):\n",
    "    \n",
    "    # All indices of discontinuous entities with more than two fragments    \n",
    "    ner_lists_raw_removed = []\n",
    "    for idx in range(len(ner_lists_raw)):\n",
    "        if len(ner_lists_raw[idx]) > 3:\n",
    "            ner_lists_raw_removed.append(idx)\n",
    "\n",
    "    # Create a dictionary to store the names and associated lists\n",
    "    dict_ner_lists_raw = {f\"T{i+1}\": ner_lists_raw[i] for i in range(len(ner_lists_raw))}\n",
    "\n",
    "    # Remove the elements in ner_lists_raw_removed from the dictionary\n",
    "    for idx in ner_lists_raw_removed:\n",
    "        del dict_ner_lists_raw[f\"T{idx+1}\"]\n",
    "\n",
    "    # Convert the dictionary items to a list\n",
    "    items = list(dict_ner_lists_raw.items())\n",
    "\n",
    "    # Create a new dictionary after deleting discontinuous entities with more than two fragments   \n",
    "    new_dict_ner_lists = {}\n",
    "    for k in range(len(ner_lists)):\n",
    "        new_dict_ner_lists[items[k][0]] = ner_lists[k]\n",
    "        \n",
    "    return new_dict_ner_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50c5d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    train[doc]['relations'] = []\n",
    "    \n",
    "    ner_lists_raw = train[doc]['ner']\n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "        \n",
    "    new_dict_ner_lists = create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists)\n",
    "    \n",
    "    \n",
    "    # Add relations\n",
    "    if 'relations' in train_re[doc]:\n",
    "\n",
    "        for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "            for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                    rel_temp = []\n",
    "                    rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                    rel_temp.append(key)\n",
    "\n",
    "            train[doc]['relations'].append(rel_temp)\n",
    "    \n",
    "    \n",
    "    if cache[doc]['remove'] != []: # Need to remove a few entities and relations\n",
    "        \n",
    "        for i in range(len(cache[doc]['remove'])):\n",
    "\n",
    "            a_num = cache[doc]['remove'][i][0][1]\n",
    "            a_name = cache[doc]['remove'][i][0][0]\n",
    "\n",
    "            b_num = cache[doc]['remove'][i][1][1]\n",
    "            b_name = cache[doc]['remove'][i][1][0]\n",
    "\n",
    "            if a_num == 0 and b_num == 0:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                \n",
    "                remove_name = cache[doc]['remove'][i][chosen_element][0]\n",
    "                remove_offsets = new_dict_ner_lists[remove_name]\n",
    "\n",
    "                train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "            elif (a_num == 0 and b_num >= 1) or (b_num == 0 and a_num >= 1):\n",
    "\n",
    "                if a_num == 0:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[a_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                elif b_num == 0:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[b_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "            elif a_num == 1 and b_num == 1:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                \n",
    "                remove_name = cache[doc]['remove'][i][chosen_element][0]\n",
    "                remove_offsets = new_dict_ner_lists[remove_name]\n",
    "\n",
    "                train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "                result = []\n",
    "                for dictionary in train_re[doc]['relations']:\n",
    "                    for key, values in dictionary.items():\n",
    "                        if remove_name in values:\n",
    "                            result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                for item in result:\n",
    "                    rel_temp = []\n",
    "                    rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                    rel_temp.append(item[2])\n",
    "\n",
    "                    train[doc]['relations'].remove(rel_temp)\n",
    "\n",
    "            else: # (a_num = 1, b_num = N) or (a_num = N, b_num = 1)\n",
    "\n",
    "\n",
    "                if a_num == 1:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[a_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                    result = []\n",
    "                    for dictionary in train_re[doc]['relations']:\n",
    "                        for key, values in dictionary.items():\n",
    "                            if a_name in values:\n",
    "                                result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                    for item in result:\n",
    "                        rel_temp = []\n",
    "                        rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                        rel_temp.append(item[2])\n",
    "\n",
    "                        train[doc]['relations'].remove(rel_temp)\n",
    "\n",
    "                elif b_num == 1:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[b_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                    result = []\n",
    "                    for dictionary in train_re[doc]['relations']:\n",
    "                        for key, values in dictionary.items():\n",
    "                            if b_name in values:\n",
    "                                result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                    for item in result:\n",
    "                        rel_temp = []\n",
    "                        rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                        rel_temp.append(item[2])\n",
    "\n",
    "                        train[doc]['relations'].remove(rel_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6920293",
   "metadata": {},
   "source": [
    "# Addjust the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0382239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i].pop('sentences')\n",
    "    train[i].pop('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e54fde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i]['sentences'] = train[i].pop('sentences_modified')\n",
    "    train[i]['ner'] = train[i].pop('ner_modified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "811d2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flattened_list = []\n",
    "    for sublist in nested_list:\n",
    "        if isinstance(sublist, list):\n",
    "            flattened_list.extend(sublist)\n",
    "        else:\n",
    "            flattened_list.append(sublist)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d4d8bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i]['sentences'] = [train[i]['sentences']]\n",
    "    train[i]['ner'] = [train[i]['ner']]\n",
    "    train[i]['relations'] = [train[i]['relations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70458297",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"/Users/XA/Desktop/Raredis/Pipeline/preprocessed_data/train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7105b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, \"w\") as f_out:\n",
    "    for line in train:\n",
    "        f_out.write(json.dumps(line))\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabab12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1aebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
