{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfaff3c",
   "metadata": {},
   "source": [
    "# Set up the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a66b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac37585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/Raredis/Pipeline/raw_data/dev/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in open(Path, 'r'):\n",
    "    train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ce62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train[i]['doc_key'] = train[i].pop('doc')\n",
    "    train[i]['sentences'] = train[i].pop('tokens')\n",
    "    train[i]['ner'] = train[i].pop('entities')\n",
    "\n",
    "    train[i]['ner_modified'] = train[i]['ner'].copy()\n",
    "    train[i]['sentences_modified'] = train[i]['sentences'].copy()\n",
    "\n",
    "    train[i].pop('start')\n",
    "    train[i].pop('end')\n",
    "    train[i].pop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2intlist(s):\n",
    "    l = []\n",
    "    l.append(int(s.split(',')[0]))\n",
    "    l.append(int(s.split(',')[1]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5a76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f422e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert number strings to number lists for ner_modified\n",
    "\n",
    "for i in range(len(train)):\n",
    "    l = []\n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        l_temp = []\n",
    "        for k in range(len(train[i]['ner_modified'][j]['span'])):\n",
    "            l_temp.append(str2intlist(train[i]['ner_modified'][j]['span'][k]))\n",
    "        l_temp.append(train[i]['ner_modified'][j]['type'])\n",
    "        l.append(l_temp)\n",
    "    train[i]['ner_modified'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c7971",
   "metadata": {},
   "source": [
    "# Remove discontinuous entities with more than 2 fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a5e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for j in range(len(train[i]['ner_modified'])):\n",
    "        if len(train[i]['ner_modified'][j]) >= 4:\n",
    "            indices_to_remove.add(j)\n",
    "            \n",
    "    new_list = [sublist for idx, sublist in enumerate(train[i]['ner_modified']) if idx not in indices_to_remove]\n",
    "    \n",
    "    train[i]['ner_modified'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a7ccc",
   "metadata": {},
   "source": [
    "# Check non-overlapped and overlapped discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of non-overlapped fragments for the current entity\n",
    "\n",
    "def num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx): \n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    if 0 < current_entitiy_idx < len(ner_lists) - 1: # Neither the 1st nor the last entity\n",
    "\n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1): # -1 excludes entity types such as 'SIGN'\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "      \n",
    "            if (previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "                and previous_entity_last_fragment_right_pos < cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    elif current_entitiy_idx == 0: # 1st entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            next_entity_first_fragment_left_pos = ner_lists[current_entitiy_idx+1][0][0]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos < next_entity_first_fragment_left_pos \n",
    "            and cur_entity_cur_fragment_right_pos < next_entity_first_fragment_left_pos):\n",
    "                num += 1\n",
    "                \n",
    "    else: # Last entity\n",
    "        \n",
    "        for k in range(len(ner_lists[current_entitiy_idx]) - 1):\n",
    "            \n",
    "            cur_entity_cur_fragment_left_pos = ner_lists[current_entitiy_idx][k][0]\n",
    "            cur_entity_cur_fragment_right_pos = ner_lists[current_entitiy_idx][k][1]\n",
    "\n",
    "            previous_entity_last_fragment_right_pos = ner_lists[current_entitiy_idx-1][-2][1]\n",
    "            \n",
    "            if (cur_entity_cur_fragment_left_pos > previous_entity_last_fragment_right_pos\n",
    "            and cur_entity_cur_fragment_right_pos > previous_entity_last_fragment_right_pos):\n",
    "                num += 1\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "    if num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx) == len(ner_lists[current_entitiy_idx]) - 1:\n",
    "        return False # All fragments of current entity do not overlap with other entities\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d772e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5th discontinuous entity in doc_0 is overlapped.\n",
      "The 6th discontinuous entity in doc_0 is overlapped.\n",
      "The 19th discontinuous entity in doc_1 is overlapped.\n",
      "The 20th discontinuous entity in doc_1 is overlapped.\n",
      "The 7th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 11th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_3 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_7 is overlapped.\n",
      "The 7th discontinuous entity in doc_7 is overlapped.\n",
      "The 3th discontinuous entity in doc_11 is non-overlapped.\n",
      "The 10th discontinuous entity in doc_13 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_14 is overlapped.\n",
      "The 20th discontinuous entity in doc_14 is overlapped.\n",
      "The 21th discontinuous entity in doc_14 is overlapped.\n",
      "The 28th discontinuous entity in doc_14 is overlapped.\n",
      "The 2th discontinuous entity in doc_17 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_17 is non-overlapped.\n",
      "The 2th discontinuous entity in doc_18 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_19 is overlapped.\n",
      "The 11th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 14th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_19 is overlapped.\n",
      "The 16th discontinuous entity in doc_19 is overlapped.\n",
      "The 25th discontinuous entity in doc_19 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_24 is overlapped.\n",
      "The 2th discontinuous entity in doc_25 is overlapped.\n",
      "The 4th discontinuous entity in doc_27 is overlapped.\n",
      "The 30th discontinuous entity in doc_29 is overlapped.\n",
      "The 31th discontinuous entity in doc_29 is overlapped.\n",
      "The 5th discontinuous entity in doc_30 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_30 is overlapped.\n",
      "The 7th discontinuous entity in doc_30 is overlapped.\n",
      "The 8th discontinuous entity in doc_30 is overlapped.\n",
      "The 4th discontinuous entity in doc_32 is non-overlapped.\n",
      "The 17th discontinuous entity in doc_32 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_33 is overlapped.\n",
      "The 6th discontinuous entity in doc_33 is overlapped.\n",
      "The 9th discontinuous entity in doc_33 is overlapped.\n",
      "The 24th discontinuous entity in doc_34 is overlapped.\n",
      "The 36th discontinuous entity in doc_34 is overlapped.\n",
      "The 5th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 14th discontinuous entity in doc_36 is overlapped.\n",
      "The 15th discontinuous entity in doc_36 is overlapped.\n",
      "The 21th discontinuous entity in doc_36 is overlapped.\n",
      "The 24th discontinuous entity in doc_36 is overlapped.\n",
      "The 25th discontinuous entity in doc_36 is overlapped.\n",
      "The 26th discontinuous entity in doc_36 is non-overlapped.\n",
      "The 28th discontinuous entity in doc_36 is overlapped.\n",
      "The 29th discontinuous entity in doc_36 is overlapped.\n",
      "The 30th discontinuous entity in doc_36 is overlapped.\n",
      "The 9th discontinuous entity in doc_39 is non-overlapped.\n",
      "The 11th discontinuous entity in doc_39 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_44 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_44 is overlapped.\n",
      "The 3th discontinuous entity in doc_45 is overlapped.\n",
      "The 13th discontinuous entity in doc_52 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_56 is non-overlapped.\n",
      "The 7th discontinuous entity in doc_59 is overlapped.\n",
      "The 14th discontinuous entity in doc_59 is non-overlapped.\n",
      "The 21th discontinuous entity in doc_59 is overlapped.\n",
      "The 14th discontinuous entity in doc_60 is overlapped.\n",
      "The 5th discontinuous entity in doc_61 is overlapped.\n",
      "The 25th discontinuous entity in doc_61 is overlapped.\n",
      "The 26th discontinuous entity in doc_61 is overlapped.\n",
      "The 27th discontinuous entity in doc_61 is overlapped.\n",
      "The 32th discontinuous entity in doc_61 is non-overlapped.\n",
      "The 1th discontinuous entity in doc_63 is non-overlapped.\n",
      "The 5th discontinuous entity in doc_63 is overlapped.\n",
      "The 5th discontinuous entity in doc_65 is overlapped.\n",
      "The 6th discontinuous entity in doc_65 is overlapped.\n",
      "The 8th discontinuous entity in doc_71 is non-overlapped.\n",
      "The 16th discontinuous entity in doc_74 is non-overlapped.\n",
      "The 0th discontinuous entity in doc_75 is overlapped.\n",
      "The 1th discontinuous entity in doc_75 is overlapped.\n",
      "The 8th discontinuous entity in doc_75 is non-overlapped.\n",
      "The 10th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 26th discontinuous entity in doc_77 is non-overlapped.\n",
      "The 4th discontinuous entity in doc_78 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_80 is overlapped.\n",
      "The 16th discontinuous entity in doc_80 is non-overlapped.\n",
      "The 18th discontinuous entity in doc_83 is overlapped.\n",
      "The 18th discontinuous entity in doc_90 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_92 is overlapped.\n",
      "The 4th discontinuous entity in doc_92 is overlapped.\n",
      "The 5th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 15th discontinuous entity in doc_96 is non-overlapped.\n",
      "The 8th discontinuous entity in doc_97 is overlapped.\n",
      "The 11th discontinuous entity in doc_97 is non-overlapped.\n",
      "The 12th discontinuous entity in doc_97 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_100 is non-overlapped.\n",
      "The 9th discontinuous entity in doc_102 is non-overlapped.\n",
      "The 5th discontinuous entity in doc_103 is overlapped.\n",
      "The 8th discontinuous entity in doc_103 is overlapped.\n",
      "The 9th discontinuous entity in doc_103 is overlapped.\n",
      "The 3th discontinuous entity in doc_104 is overlapped.\n",
      "The 4th discontinuous entity in doc_104 is overlapped.\n",
      "The 5th discontinuous entity in doc_104 is overlapped.\n",
      "The 8th discontinuous entity in doc_104 is overlapped.\n",
      "The 4th discontinuous entity in doc_108 is overlapped.\n",
      "The 9th discontinuous entity in doc_108 is overlapped.\n",
      "The 10th discontinuous entity in doc_108 is non-overlapped.\n",
      "The 13th discontinuous entity in doc_108 is non-overlapped.\n",
      "The 2th discontinuous entity in doc_110 is overlapped.\n",
      "The 2th discontinuous entity in doc_112 is non-overlapped.\n",
      "The 6th discontinuous entity in doc_114 is non-overlapped.\n",
      "The 3th discontinuous entity in doc_118 is overlapped.\n",
      "The 4th discontinuous entity in doc_118 is overlapped.\n",
      "The 5th discontinuous entity in doc_118 is overlapped.\n",
      "The 9th discontinuous entity in doc_125 is overlapped.\n",
      "The 10th discontinuous entity in doc_125 is overlapped.\n",
      "The 24th discontinuous entity in doc_126 is overlapped.\n",
      "The 2th discontinuous entity in doc_127 is overlapped.\n"
     ]
    }
   ],
   "source": [
    "num_non_overlapped = 0\n",
    "num_overlapped = 0\n",
    "\n",
    "for doc in range(len(train)):\n",
    "    \n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "    \n",
    "    for current_entitiy_idx in range(len(ner_lists)): \n",
    "        \n",
    "        if len(ner_lists[current_entitiy_idx]) == 3: # 2-fragment discontinuous entities\n",
    "\n",
    "            num_nonoverlap_fragments = num_nonoverlap_fragments_current_entity(ner_lists, current_entitiy_idx)\n",
    "\n",
    "            if not check_cur_entity_overlap(ner_lists, current_entitiy_idx, num_nonoverlap_fragments):\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is non-overlapped.\")\n",
    "                num_non_overlapped += 1\n",
    "            else:\n",
    "                print(f\"The {current_entitiy_idx}th discontinuous entity in doc_{doc} is overlapped.\")\n",
    "                num_overlapped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4315bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_non_overlapped is 46 and num_overlapped is 70\n"
     ]
    }
   ],
   "source": [
    "print(f'num_non_overlapped is {num_non_overlapped} and num_overlapped is {num_overlapped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da3b0a",
   "metadata": {},
   "source": [
    "# Check whether two fragments are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0465c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(fragment1, fragment2):\n",
    "    \n",
    "    # Extract the left and right positions of each fragment\n",
    "    left1, right1 = fragment1\n",
    "    left2, right2 = fragment2\n",
    "\n",
    "    # Check for overlap\n",
    "    if right1 < left2 or right2 < left1:\n",
    "        return False  # Non-overlapping fragments\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07348755",
   "metadata": {},
   "source": [
    "# Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81658a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 2nd fragment to the right of the 1st fragment\n",
    "\n",
    "def modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (including) the last token of the 1st fragment\n",
    "    tokens_before_1st_fragment = tokens[:ner_lists[current_entitiy_idx][0][1] + 1]\n",
    "    # All tokens of the 2nd fragment\n",
    "    tokens_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:ner_lists[current_entitiy_idx][1][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][0][1] + 1:]\n",
    "    \n",
    "    new_tokens = tokens_before_1st_fragment + tokens_2nd_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b00c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_1(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 2nd fragment\n",
    "    len_2nd_fragment = ner_lists[current_entitiy_idx][1][1] - ner_lists[current_entitiy_idx][1][0] + 1\n",
    "\n",
    "    last_token_pos_1st_fragment = ner_lists[current_entitiy_idx][0][1]\n",
    "    \n",
    "    # Modify offsets for the current discontinuous entity \n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][0][0], \n",
    "                                       ner_lists[current_entitiy_idx][0][1] + len_2nd_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)): \n",
    "\n",
    "        if idx != current_entitiy_idx:\n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "                \n",
    "                if (ner_lists[idx][k][0] > last_token_pos_1st_fragment \n",
    "                and ner_lists[idx][k][1] > last_token_pos_1st_fragment):\n",
    "                    ner_lists[idx][k][0] += len_2nd_fragment\n",
    "                    ner_lists[idx][k][1] += len_2nd_fragment   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3d48",
   "metadata": {},
   "source": [
    "# Rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785a881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 1st fragment to the left of the 2nd fragment\n",
    "\n",
    "def modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # All tokens before (excluding) the 1st token of the 2nd fragment\n",
    "    tokens_before_2nd_fragment = tokens[:ner_lists[current_entitiy_idx][1][0]]\n",
    "    # All tokens of the 1st fragment\n",
    "    tokens_1st_fragment = tokens[ner_lists[current_entitiy_idx][0][0]:ner_lists[current_entitiy_idx][0][1] + 1] \n",
    "    # All tokens after (including) the 1st token of the 2nd fragment\n",
    "    tokens_after_2nd_fragment = tokens[ner_lists[current_entitiy_idx][1][0]:]\n",
    "    \n",
    "    new_tokens = tokens_before_2nd_fragment + tokens_1st_fragment + tokens_after_2nd_fragment\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc176f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_offsets_rule_2(ner_lists, current_entitiy_idx):\n",
    "    \n",
    "    # Number of tokens of the 1st fragment\n",
    "    len_1st_fragment = ner_lists[current_entitiy_idx][0][1] - ner_lists[current_entitiy_idx][0][0] + 1\n",
    "\n",
    "    first_token_pos_2nd_fragment = ner_lists[current_entitiy_idx][1][0]\n",
    "\n",
    "    ner_lists[current_entitiy_idx] = [[ner_lists[current_entitiy_idx][1][0], \n",
    "                                       ner_lists[current_entitiy_idx][1][1] + len_1st_fragment], \n",
    "                                      ner_lists[current_entitiy_idx][-1]]\n",
    "\n",
    "    for idx in range(len(ner_lists)):         \n",
    "\n",
    "        if idx != current_entitiy_idx:    \n",
    "\n",
    "            for k in range(len(ner_lists[idx]) - 1):\n",
    "\n",
    "                if (ner_lists[idx][k][0] >= first_token_pos_2nd_fragment \n",
    "                and ner_lists[idx][k][1] >= first_token_pos_2nd_fragment):\n",
    "                    ner_lists[idx][k][0] += len_1st_fragment\n",
    "                    ner_lists[idx][k][1] += len_1st_fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152b00e",
   "metadata": {},
   "source": [
    "# Modify sentences with 2-fragment discontinuous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13baf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    tokens = train[doc]['sentences_modified'] \n",
    "    # Changing tokens will NOT change train[doc]['sentences_modified'] for *operations* below\n",
    "    ner_lists = train[doc]['ner_modified'] \n",
    "    # Changing ner_lists WILL change train[doc]['ner_modified'] for *operations* below\n",
    "\n",
    "    for current_entitiy_idx in range(len(ner_lists)):\n",
    "\n",
    "        if len(ner_lists[current_entitiy_idx]) == 3:  # 2-fragment discontinuous entities\n",
    "            \n",
    "            interval_1st = ner_lists[current_entitiy_idx][0]\n",
    "            interval_2nd = ner_lists[current_entitiy_idx][1]\n",
    "\n",
    "            switch = 0\n",
    "\n",
    "            for idx in range(len(ner_lists)):\n",
    "\n",
    "                if idx != current_entitiy_idx:\n",
    "\n",
    "                    if len(ner_lists[idx]) == 2: # Continuous entities\n",
    "                        \n",
    "                        interval_cont = ner_lists[idx][0] \n",
    "                        \n",
    "                        if check_overlap(interval_2nd, interval_cont):\n",
    "                            switch = 1\n",
    "                            break\n",
    "                        \n",
    "                        elif check_overlap(interval_1st, interval_cont):\n",
    "                            switch = 3\n",
    "                            break\n",
    "                               \n",
    "                        \n",
    "                    elif len(ner_lists[idx]) == 3: # 2-fragment discontinuous entities\n",
    "                        \n",
    "                        interval_discont_1st = ner_lists[idx][0]\n",
    "                        interval_discont_2nd = ner_lists[idx][1]\n",
    "                                              \n",
    "                        if (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is False):\n",
    "                            switch = 2\n",
    "                            break\n",
    "\n",
    "                        elif (check_overlap(interval_1st, interval_discont_1st) is True \n",
    "                        and check_overlap(interval_2nd, interval_discont_2nd) is False):\n",
    "                            switch = 4\n",
    "                            break                             \n",
    "                            \n",
    "                        elif (check_overlap(interval_2nd, interval_discont_2nd) is True \n",
    "                        and check_overlap(interval_1st, interval_discont_1st) is True\n",
    "                           and interval_1st[1] > interval_discont_1st[1]\n",
    "                           and interval_2nd[0] > interval_discont_1st[0]):\n",
    "                            switch = 5\n",
    "                            break\n",
    "                            \n",
    "                        \n",
    "            if switch == 1 or switch == 2 or switch == 5:\n",
    "\n",
    "                tokens = modify_tokens_rule_1(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_1(ner_lists, current_entitiy_idx)\n",
    "                \n",
    "                \n",
    "            elif switch == 0 or switch == 3 or switch == 4:\n",
    "\n",
    "                tokens = modify_tokens_rule_2(tokens, ner_lists, current_entitiy_idx)\n",
    "                train[doc]['sentences_modified'] = tokens\n",
    "                \n",
    "                modify_offsets_rule_2(ner_lists, current_entitiy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39ffdb",
   "metadata": {},
   "source": [
    "# Sanity check that all entities are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    l_modified = []\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        l_modified.append(train[doc]['sentences_modified'][train[doc]['ner_modified'][i][0][0]:train[doc]['ner_modified'][i][0][1] + 1])\n",
    "\n",
    "    l_original = []\n",
    "    for i in range(len(train[doc]['ner'])):\n",
    "        if len(train[doc]['ner'][i]) == 2:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1])\n",
    "        elif len(train[doc]['ner'][i]) == 3:\n",
    "            l_original.append(train[doc]['sentences'][train[doc]['ner'][i][0][0]:train[doc]['ner'][i][0][1]+1] + train[doc]['sentences'][train[doc]['ner'][i][1][0]:train[doc]['ner'][i][1][1]+1])\n",
    "        else:\n",
    "            pass        \n",
    "        \n",
    "    if l_modified != l_original:          \n",
    "        print(f'The doc_{doc} needs more examination.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ed29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a132bef7",
   "metadata": {},
   "source": [
    "# Add relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3a42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = '/Users/XA/Desktop/Raredis/Pipeline/raw_data/dev/dev_re.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0a05ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_re = []\n",
    "for line in open(Path, 'r'):\n",
    "    train_re.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f6d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists):\n",
    "    \n",
    "    # All indices of discontinuous entities with more than two fragments    \n",
    "    ner_lists_raw_removed = []\n",
    "    for idx in range(len(ner_lists_raw)):\n",
    "        if len(ner_lists_raw[idx]) > 3:\n",
    "            ner_lists_raw_removed.append(idx)\n",
    "\n",
    "    # Create a dictionary to store the names and associated lists\n",
    "    dict_ner_lists_raw = {f\"T{i+1}\": ner_lists_raw[i] for i in range(len(ner_lists_raw))}\n",
    "\n",
    "    # Remove the elements in ner_lists_raw_removed from the dictionary\n",
    "    for idx in ner_lists_raw_removed:\n",
    "        del dict_ner_lists_raw[f\"T{idx+1}\"]\n",
    "\n",
    "    # Convert the dictionary items to a list\n",
    "    items = list(dict_ner_lists_raw.items())\n",
    "\n",
    "    # Create a new dictionary after deleting discontinuous entities with more than two fragments   \n",
    "    new_dict_ner_lists = {}\n",
    "    for k in range(len(ner_lists)):\n",
    "        new_dict_ner_lists[items[k][0]] = ner_lists[k]\n",
    "        \n",
    "    return new_dict_ner_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ad2028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 5 does not contain any relation.\n",
      "Doc 15 does not contain any relation.\n",
      "Doc 16 does not contain any relation.\n",
      "Doc 20 does not contain any relation.\n",
      "Doc 22 does not contain any relation.\n",
      "Doc 37 does not contain any relation.\n",
      "Doc 86 does not contain any relation.\n",
      "Doc 121 does not contain any relation.\n"
     ]
    }
   ],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    train[doc]['relations'] = []\n",
    "    \n",
    "    ner_lists_raw = train[doc]['ner']\n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "        \n",
    "    new_dict_ner_lists = create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists)\n",
    "        \n",
    "    if 'relations' in train_re[doc]:\n",
    "        \n",
    "        for i in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "            for key, value in train_re[doc]['relations'][i].items():\n",
    "\n",
    "                if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                    rel_temp = []\n",
    "                    rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                    rel_temp.append(key) \n",
    "\n",
    "            train[doc]['relations'].append(rel_temp)\n",
    "            \n",
    "    else:\n",
    "        print(f'Doc {doc} does not contain any relation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bd86e",
   "metadata": {},
   "source": [
    "# Cache entities with double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332bba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are ['T2', 'T3'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T2 has number 0\n",
      "T3 has number 0\n",
      "\n",
      "\n",
      "keys are ['T4', 'T5'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T4 has number 0\n",
      "T5 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T11 has number 0\n",
      "T12 has number 1\n",
      "\n",
      "\n",
      "keys are ['T13', 'T14'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T13 has number 0\n",
      "T14 has number 1\n",
      "\n",
      "\n",
      "keys are ['T16', 'T17'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T16 has number 0\n",
      "T17 has number 1\n",
      "\n",
      "\n",
      "keys are ['T18', 'T19'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T18 has number 0\n",
      "T19 has number 1\n",
      "\n",
      "\n",
      "keys are ['T24', 'T25'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T24 has number 0\n",
      "T25 has number 1\n",
      "\n",
      "\n",
      "keys are ['T26', 'T27'] and doc is SLC6A1-Epileptic-Encephalopathy_dev1\n",
      "T26 has number 0\n",
      "T27 has number 1\n",
      "\n",
      "\n",
      "keys are ['T6', 'T7'] and doc is Tooth-and-Nail-Syndrome_dev3\n",
      "T6 has number 0\n",
      "T7 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is Sialadenitis_dev6\n",
      "T11 has number 0\n",
      "T12 has number 1\n",
      "\n",
      "\n",
      "keys are ['T5', 'T6'] and doc is Urticaria-Cold_dev7\n",
      "T5 has number 0\n",
      "T6 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Vogt_dev12\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T7', 'T8'] and doc is WAGR-Syndrome-11p-Deletion-Syndrome_dev13\n",
      "T7 has number 1\n",
      "T8 has number 1\n",
      "\n",
      "\n",
      "keys are ['T15', 'T16'] and doc is Valinemia_dev14\n",
      "T15 has number 0\n",
      "T16 has number 1\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Trichorhinophalangeal-Syndrome-Type-I_dev19\n",
      "T9 has number 0\n",
      "T10 has number 13\n",
      "\n",
      "\n",
      "keys are ['T12', 'T13'] and doc is Truncus-Arteriosus_dev21\n",
      "T12 has number 0\n",
      "T13 has number 1\n",
      "\n",
      "\n",
      "keys are ['T18', 'T19'] and doc is Williams-Syndrome_dev23\n",
      "T18 has number 0\n",
      "T19 has number 1\n",
      "\n",
      "\n",
      "keys are ['T5', 'T6'] and doc is Smith-Lemli-Opitz-Syndrome_dev24\n",
      "T5 has number 0\n",
      "T6 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is Smith-Lemli-Opitz-Syndrome_dev24\n",
      "T11 has number 0\n",
      "T12 has number 1\n",
      "\n",
      "\n",
      "keys are ['T5', 'T6'] and doc is Scott-Craniodigital-Syndrome_dev25\n",
      "T5 has number 0\n",
      "T6 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Split-Hand-Split-Foot-Malformation_dev33\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T16', 'T17'] and doc is WAS-Related-Disorders_dev34\n",
      "T16 has number 1\n",
      "T17 has number 1\n",
      "\n",
      "\n",
      "keys are ['T18', 'T19'] and doc is WAS-Related-Disorders_dev34\n",
      "T18 has number 1\n",
      "T19 has number 1\n",
      "\n",
      "\n",
      "keys are ['T23', 'T24'] and doc is WAS-Related-Disorders_dev34\n",
      "T23 has number 0\n",
      "T24 has number 1\n",
      "\n",
      "\n",
      "keys are ['T27', 'T28'] and doc is WAS-Related-Disorders_dev34\n",
      "T27 has number 1\n",
      "T28 has number 1\n",
      "\n",
      "\n",
      "keys are ['T16', 'T17'] and doc is Snyder_dev39\n",
      "T16 has number 0\n",
      "T17 has number 1\n",
      "\n",
      "\n",
      "keys are ['T7', 'T8'] and doc is Tyrosine-Hydroxylase-Deficiency_dev44\n",
      "T7 has number 0\n",
      "T8 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is Tethered-Cord-Syndrome_dev45\n",
      "T11 has number 0\n",
      "T12 has number 1\n",
      "\n",
      "\n",
      "keys are ['T14', 'T15'] and doc is West-Nile-Encephalitis_dev47\n",
      "T14 has number 0\n",
      "T15 has number 1\n",
      "\n",
      "\n",
      "keys are ['T16', 'T17'] and doc is West-Nile-Encephalitis_dev47\n",
      "T16 has number 0\n",
      "T17 has number 1\n",
      "\n",
      "\n",
      "keys are ['T10', 'T11'] and doc is Trichorhinophalangeal-Syndrome-Type-II_dev52\n",
      "T10 has number 13\n",
      "T11 has number 0\n",
      "\n",
      "\n",
      "keys are ['T18', 'T19'] and doc is Trichorhinophalangeal-Syndrome-Type-II_dev52\n",
      "T18 has number 1\n",
      "T19 has number 0\n",
      "\n",
      "\n",
      "keys are ['T20', 'T21'] and doc is Trichorhinophalangeal-Syndrome-Type-II_dev52\n",
      "T20 has number 1\n",
      "T21 has number 0\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Wieacker-Syndrome_dev53\n",
      "T9 has number 0\n",
      "T10 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is Wieacker-Syndrome_dev53\n",
      "T11 has number 0\n",
      "T12 has number 2\n",
      "\n",
      "\n",
      "keys are ['T28', 'T29'] and doc is Wieacker-Syndrome_dev53\n",
      "T28 has number 0\n",
      "T29 has number 1\n",
      "\n",
      "\n",
      "keys are ['T30', 'T31'] and doc is Wieacker-Syndrome_dev53\n",
      "T30 has number 0\n",
      "T31 has number 1\n",
      "\n",
      "\n",
      "keys are ['T10', 'T11'] and doc is Shwachman-Diamond-Syndrome_dev56\n",
      "T10 has number 0\n",
      "T11 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Yunis-Varon-Syndrome_dev60\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T24', 'T25'] and doc is Yunis-Varon-Syndrome_dev60\n",
      "T24 has number 0\n",
      "T25 has number 1\n",
      "\n",
      "\n",
      "keys are ['T10', 'T11'] and doc is Tangier-Disease_dev61\n",
      "T10 has number 0\n",
      "T11 has number 1\n",
      "\n",
      "\n",
      "keys are ['T13', 'T14'] and doc is Tangier-Disease_dev61\n",
      "T13 has number 0\n",
      "T14 has number 1\n",
      "\n",
      "\n",
      "keys are ['T31', 'T32'] and doc is Tangier-Disease_dev61\n",
      "T31 has number 0\n",
      "T32 has number 1\n",
      "\n",
      "\n",
      "keys are ['T36', 'T37'] and doc is Tangier-Disease_dev61\n",
      "T36 has number 0\n",
      "T37 has number 1\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Spastic-Paraplegia-50_dev74\n",
      "T9 has number 0\n",
      "T10 has number 1\n",
      "\n",
      "\n",
      "keys are ['T18', 'T19'] and doc is Spastic-Paraplegia-50_dev74\n",
      "T18 has number 0\n",
      "T19 has number 1\n",
      "\n",
      "\n",
      "keys are ['T27', 'T28'] and doc is Spastic-Paraplegia-50_dev74\n",
      "T27 has number 0\n",
      "T28 has number 1\n",
      "\n",
      "\n",
      "keys are ['T41', 'T42'] and doc is Spastic-Paraplegia-50_dev74\n",
      "T41 has number 0\n",
      "T42 has number 1\n",
      "\n",
      "\n",
      "keys are ['T14', 'T15'] and doc is Tricho-Dento-Osseous-Syndrome_dev75\n",
      "T14 has number 0\n",
      "T15 has number 1\n",
      "\n",
      "\n",
      "keys are ['T5', 'T6'] and doc is Wildervanck-Syndrome_dev77\n",
      "T5 has number 1\n",
      "T6 has number 1\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Wildervanck-Syndrome_dev77\n",
      "T9 has number 0\n",
      "T10 has number 1\n",
      "\n",
      "\n",
      "keys are ['T12', 'T13'] and doc is Wildervanck-Syndrome_dev77\n",
      "T12 has number 1\n",
      "T13 has number 1\n",
      "\n",
      "\n",
      "keys are ['T15', 'T16'] and doc is Wildervanck-Syndrome_dev77\n",
      "T15 has number 3\n",
      "T16 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Urticaria-Papular_dev78\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T12', 'T13'] and doc is Urticaria-Papular_dev78\n",
      "T12 has number 1\n",
      "T13 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Spastic-Paraplegia-51_dev81\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T22', 'T23'] and doc is Spastic-Paraplegia-51_dev81\n",
      "T22 has number 0\n",
      "T23 has number 1\n",
      "\n",
      "\n",
      "keys are ['T6', 'T7'] and doc is Simian-B-Virus-Infection_dev83\n",
      "T6 has number 0\n",
      "T7 has number 1\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Simian-B-Virus-Infection_dev83\n",
      "T9 has number 0\n",
      "T10 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Sotos-Syndrome_dev88\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T3', 'T4'] and doc is Werner-Syndrome_dev89\n",
      "T3 has number 1\n",
      "T4 has number 0\n",
      "\n",
      "\n",
      "keys are ['T6', 'T7'] and doc is Sneddon-Syndrome_dev90\n",
      "T6 has number 2\n",
      "T7 has number 1\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Sneddon-Syndrome_dev90\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T10', 'T11'] and doc is Sneddon-Syndrome_dev90\n",
      "T10 has number 0\n",
      "T11 has number 1\n",
      "\n",
      "\n",
      "keys are ['T15', 'T16'] and doc is Sneddon-Syndrome_dev90\n",
      "T15 has number 0\n",
      "T16 has number 1\n",
      "\n",
      "\n",
      "keys are ['T17', 'T18'] and doc is Sneddon-Syndrome_dev90\n",
      "T17 has number 0\n",
      "T18 has number 1\n",
      "\n",
      "\n",
      "keys are ['T25', 'T26'] and doc is Sneddon-Syndrome_dev90\n",
      "T25 has number 0\n",
      "T26 has number 0\n",
      "\n",
      "\n",
      "keys are ['T8', 'T9'] and doc is Spastic-Paraplegia-52_dev91\n",
      "T8 has number 0\n",
      "T9 has number 1\n",
      "\n",
      "\n",
      "keys are ['T22', 'T23'] and doc is Spastic-Paraplegia-52_dev91\n",
      "T22 has number 0\n",
      "T23 has number 1\n",
      "\n",
      "\n",
      "keys are ['T3', 'T4'] and doc is Zollinger_dev93\n",
      "T3 has number 0\n",
      "T4 has number 1\n",
      "\n",
      "\n",
      "keys are ['T5', 'T6'] and doc is Zollinger_dev93\n",
      "T5 has number 0\n",
      "T6 has number 1\n",
      "\n",
      "\n",
      "keys are ['T4', 'T5'] and doc is Senior-Lken-Syndrome_dev94\n",
      "T4 has number 0\n",
      "T5 has number 1\n",
      "\n",
      "\n",
      "keys are ['T6', 'T7'] and doc is Senior-Lken-Syndrome_dev94\n",
      "T6 has number 0\n",
      "T7 has number 1\n",
      "\n",
      "\n",
      "keys are ['T15', 'T16'] and doc is Trichorhinophalangeal-Syndrome-Type-III_dev97\n",
      "T15 has number 0\n",
      "T16 has number 1\n",
      "\n",
      "\n",
      "keys are ['T6', 'T7'] and doc is Wolf_dev102\n",
      "T6 has number 0\n",
      "T7 has number 1\n",
      "\n",
      "\n",
      "keys are ['T11', 'T12'] and doc is Wolf_dev102\n",
      "T11 has number 0\n",
      "T12 has number 1\n",
      "\n",
      "\n",
      "keys are ['T9', 'T10'] and doc is Vernal-Keratonconjunctivitis_dev107\n",
      "T9 has number 0\n",
      "T10 has number 1\n",
      "\n",
      "\n",
      "keys are ['T10', 'T11'] and doc is Weill-Marchesani-Syndrome_dev114\n",
      "T10 has number 0\n",
      "T11 has number 1\n",
      "\n",
      "\n",
      "keys are ['T13', 'T14'] and doc is Weill-Marchesani-Syndrome_dev114\n",
      "T13 has number 0\n",
      "T14 has number 1\n",
      "\n",
      "\n",
      "keys are ['T21', 'T22'] and doc is Systemic-Capillary-Leak-Syndrome_dev126\n",
      "T21 has number 0\n",
      "T22 has number 1\n",
      "\n",
      "\n",
      "keys are ['T13', 'T14'] and doc is Simpson-Dysmorphia-Syndrome_dev127\n",
      "T13 has number 0\n",
      "T14 has number 1\n",
      "\n",
      "\n",
      "keys are ['T15', 'T16'] and doc is Simpson-Dysmorphia-Syndrome_dev127\n",
      "T15 has number 0\n",
      "T16 has number 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for doc in range(len(train_re)):\n",
    "    \n",
    "    temp = {}\n",
    "    doc_name = train_re[doc]['doc']\n",
    "    temp['doc'] = doc_name\n",
    "    temp['remove'] = []\n",
    "    \n",
    "    # dictionary to store offsets and their keys\n",
    "    offset_keys = {}\n",
    "\n",
    "    # iterate over the list\n",
    "    for item in train_re[doc]['entities']:\n",
    "        for k, v in item.items():\n",
    "            \n",
    "            if len(v) == 2:\n",
    "            \n",
    "                offset = tuple(v[0])  # use tuple to make the list hashable\n",
    "                # store the key instead of the type\n",
    "                if offset not in offset_keys:\n",
    "                    offset_keys[offset] = [k]\n",
    "                else:\n",
    "                    offset_keys[offset].append(k)\n",
    "\n",
    "    # find and print the offsets with more than one key\n",
    "    for offset, keys in offset_keys.items():\n",
    "        \n",
    "        if len(keys) > 1:\n",
    "            \n",
    "            a = 0\n",
    "            b = 0\n",
    "\n",
    "            for l in train_re[doc]['relations']:\n",
    "\n",
    "                for k, v in l.items():\n",
    "                    if keys[0] in v:\n",
    "                        a += 1\n",
    "                    elif keys[1] in v:\n",
    "                        b += 1\n",
    "\n",
    "            print(f'keys are {keys} and doc is {doc_name}')\n",
    "\n",
    "            print(f'{keys[0]} has number {a}')\n",
    "            print(f'{keys[1]} has number {b}')\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "            l = []\n",
    "\n",
    "            l.append([keys[0], a])\n",
    "            l.append([keys[1], b])\n",
    "\n",
    "            temp['remove'].append(l)\n",
    "                \n",
    "                \n",
    "    res.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d827312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': 'Sporadic-Inclusion-Body-Myositis_dev0', 'remove': []}\n",
      "{'doc': 'SLC6A1-Epileptic-Encephalopathy_dev1', 'remove': [[['T2', 0], ['T3', 0]], [['T4', 0], ['T5', 1]], [['T8', 0], ['T9', 1]], [['T11', 0], ['T12', 1]], [['T13', 0], ['T14', 1]], [['T16', 0], ['T17', 1]], [['T18', 0], ['T19', 1]], [['T24', 0], ['T25', 1]], [['T26', 0], ['T27', 1]]]}\n",
      "{'doc': 'TORCH-Syndrome_dev2', 'remove': []}\n",
      "{'doc': 'Tooth-and-Nail-Syndrome_dev3', 'remove': [[['T6', 0], ['T7', 1]]]}\n",
      "{'doc': 'Trisomy-13-Syndrome_dev4', 'remove': []}\n",
      "{'doc': 'Tularemia_dev5', 'remove': []}\n",
      "{'doc': 'Sialadenitis_dev6', 'remove': [[['T11', 0], ['T12', 1]]]}\n",
      "{'doc': 'Urticaria-Cold_dev7', 'remove': [[['T5', 0], ['T6', 1]]]}\n",
      "{'doc': 'Simple-Pulmonary-Eosinophilia_dev8', 'remove': []}\n",
      "{'doc': 'Syringobulbia_dev9', 'remove': []}\n",
      "{'doc': 'Short-Chain-Acyl-CoA-Dehydrogenase-Deficiency_dev10', 'remove': []}\n",
      "{'doc': 'Weismann-Netter-Stuhl-Syndrome_dev11', 'remove': []}\n",
      "{'doc': 'Vogt_dev12', 'remove': [[['T8', 0], ['T9', 1]]]}\n",
      "{'doc': 'WAGR-Syndrome-11p-Deletion-Syndrome_dev13', 'remove': [[['T7', 1], ['T8', 1]]]}\n",
      "{'doc': 'Valinemia_dev14', 'remove': [[['T15', 0], ['T16', 1]]]}\n",
      "{'doc': 'Systemic-Primary-Carnitine-Deficiency_dev15', 'remove': []}\n",
      "{'doc': 'Spondylothoracic-Dysplasia_dev16', 'remove': []}\n",
      "{'doc': 'Tay-Sachs-Disease_dev17', 'remove': []}\n",
      "{'doc': 'Urticaria-Physical_dev18', 'remove': []}\n",
      "{'doc': 'Trichorhinophalangeal-Syndrome-Type-I_dev19', 'remove': [[['T9', 0], ['T10', 13]]]}\n",
      "{'doc': 'Vasculitis_dev20', 'remove': []}\n",
      "{'doc': 'Truncus-Arteriosus_dev21', 'remove': [[['T12', 0], ['T13', 1]]]}\n",
      "{'doc': 'Syringomyelia_dev22', 'remove': []}\n",
      "{'doc': 'Williams-Syndrome_dev23', 'remove': [[['T18', 0], ['T19', 1]]]}\n",
      "{'doc': 'Smith-Lemli-Opitz-Syndrome_dev24', 'remove': [[['T5', 0], ['T6', 1]], [['T11', 0], ['T12', 1]]]}\n",
      "{'doc': 'Scott-Craniodigital-Syndrome_dev25', 'remove': [[['T5', 0], ['T6', 1]]]}\n",
      "{'doc': 'Wilms_-Tumor_dev26', 'remove': []}\n",
      "{'doc': 'Weil-Syndrome_dev27', 'remove': []}\n",
      "{'doc': 'Weaver-Syndrome_dev28', 'remove': []}\n",
      "{'doc': 'Superior-Semicircular-Canal-Dehiscence_dev29', 'remove': []}\n",
      "{'doc': 'Twin_dev30', 'remove': []}\n",
      "{'doc': 'Warm-Antibody-Hemolytic-Anemia_dev31', 'remove': []}\n",
      "{'doc': 'Seckel-Syndrome_dev32', 'remove': []}\n",
      "{'doc': 'Split-Hand-Split-Foot-Malformation_dev33', 'remove': [[['T8', 0], ['T9', 1]]]}\n",
      "{'doc': 'WAS-Related-Disorders_dev34', 'remove': [[['T16', 1], ['T17', 1]], [['T18', 1], ['T19', 1]], [['T23', 0], ['T24', 1]], [['T27', 1], ['T28', 1]]]}\n",
      "{'doc': 'Sickle-Cell-Disease_dev35', 'remove': []}\n",
      "{'doc': 'Setleis-Syndrome_dev36', 'remove': []}\n",
      "{'doc': 'Wolfram-Syndrome_dev37', 'remove': []}\n",
      "{'doc': 'WNT4-Deficiency_dev38', 'remove': []}\n",
      "{'doc': 'Snyder_dev39', 'remove': [[['T16', 0], ['T17', 1]]]}\n",
      "{'doc': 'Whipple-Disease_dev40', 'remove': []}\n",
      "{'doc': 'Susac-Syndrome_dev41', 'remove': []}\n",
      "{'doc': 'Shprintzen-Goldberg-Syndrome_dev42', 'remove': []}\n",
      "{'doc': 'Syphilis-Acquired_dev43', 'remove': []}\n",
      "{'doc': 'Tyrosine-Hydroxylase-Deficiency_dev44', 'remove': [[['T7', 0], ['T8', 1]]]}\n",
      "{'doc': 'Tethered-Cord-Syndrome_dev45', 'remove': [[['T11', 0], ['T12', 1]]]}\n",
      "{'doc': 'Tuberculosis_dev46', 'remove': []}\n",
      "{'doc': 'West-Nile-Encephalitis_dev47', 'remove': [[['T14', 0], ['T15', 1]], [['T16', 0], ['T17', 1]]]}\n",
      "{'doc': 'Tenosynovial-Giant-Cell-Tumor_dev48', 'remove': []}\n",
      "{'doc': 'Wolff-Parkinson-White-Syndrome_dev49', 'remove': []}\n",
      "{'doc': 'Wiedemann-Rautenstrauch-Syndrome_dev50', 'remove': []}\n",
      "{'doc': 'Tropical-Sprue_dev51', 'remove': []}\n",
      "{'doc': 'Trichorhinophalangeal-Syndrome-Type-II_dev52', 'remove': [[['T10', 13], ['T11', 0]], [['T18', 1], ['T19', 0]], [['T20', 1], ['T21', 0]]]}\n",
      "{'doc': 'Wieacker-Syndrome_dev53', 'remove': [[['T9', 0], ['T10', 1]], [['T11', 0], ['T12', 2]], [['T28', 0], ['T29', 1]], [['T30', 0], ['T31', 1]]]}\n",
      "{'doc': 'Wandering-Spleen_dev54', 'remove': []}\n",
      "{'doc': 'Trigeminal-Neuralgia_dev55', 'remove': []}\n",
      "{'doc': 'Shwachman-Diamond-Syndrome_dev56', 'remove': [[['T10', 0], ['T11', 1]]]}\n",
      "{'doc': 'Three-M-Syndrome_dev57', 'remove': []}\n",
      "{'doc': 'Sudden-Infant-Death-Syndrome_dev58', 'remove': []}\n",
      "{'doc': 'Sweet-Syndrome_dev59', 'remove': []}\n",
      "{'doc': 'Yunis-Varon-Syndrome_dev60', 'remove': [[['T8', 0], ['T9', 1]], [['T24', 0], ['T25', 1]]]}\n",
      "{'doc': 'Tangier-Disease_dev61', 'remove': [[['T10', 0], ['T11', 1]], [['T13', 0], ['T14', 1]], [['T31', 0], ['T32', 1]], [['T36', 0], ['T37', 1]]]}\n",
      "{'doc': 'Spondyloepiphyseal-Dysplasia-Tarda_dev62', 'remove': []}\n",
      "{'doc': 'Spina-Bifida_dev63', 'remove': []}\n",
      "{'doc': 'Toxic-Shock-Syndrome_dev64', 'remove': []}\n",
      "{'doc': 'Severe-Combined-Immunodeficiency_dev65', 'remove': []}\n",
      "{'doc': 'Sinonasal-Undifferentiated-Carcinoma_dev66', 'remove': []}\n",
      "{'doc': 'Tooth-Agenesis_dev67', 'remove': []}\n",
      "{'doc': 'Transverse-Myelitis_dev68', 'remove': []}\n",
      "{'doc': 'Urachal-Cancer_dev69', 'remove': []}\n",
      "{'doc': 'Uterine-Leiomyosarcoma_dev70', 'remove': []}\n",
      "{'doc': 'Wernicke_dev71', 'remove': []}\n",
      "{'doc': 'Status-Epilepticus_dev72', 'remove': []}\n",
      "{'doc': 'Tuberous-Sclerosis_dev73', 'remove': []}\n",
      "{'doc': 'Spastic-Paraplegia-50_dev74', 'remove': [[['T9', 0], ['T10', 1]], [['T18', 0], ['T19', 1]], [['T27', 0], ['T28', 1]], [['T41', 0], ['T42', 1]]]}\n",
      "{'doc': 'Tricho-Dento-Osseous-Syndrome_dev75', 'remove': [[['T14', 0], ['T15', 1]]]}\n",
      "{'doc': 'Sjgren_dev76', 'remove': []}\n",
      "{'doc': 'Wildervanck-Syndrome_dev77', 'remove': [[['T5', 1], ['T6', 1]], [['T9', 0], ['T10', 1]], [['T12', 1], ['T13', 1]], [['T15', 3], ['T16', 1]]]}\n",
      "{'doc': 'Urticaria-Papular_dev78', 'remove': [[['T8', 0], ['T9', 1]], [['T12', 1], ['T13', 1]]]}\n",
      "{'doc': 'Tarlov-Cysts_dev79', 'remove': []}\n",
      "{'doc': 'Winchester-Syndrome_dev80', 'remove': []}\n",
      "{'doc': 'Spastic-Paraplegia-51_dev81', 'remove': [[['T8', 0], ['T9', 1]], [['T22', 0], ['T23', 1]]]}\n",
      "{'doc': 'Smallpox_dev82', 'remove': []}\n",
      "{'doc': 'Simian-B-Virus-Infection_dev83', 'remove': [[['T6', 0], ['T7', 1]], [['T9', 0], ['T10', 1]]]}\n",
      "{'doc': 'Typhoid_dev84', 'remove': []}\n",
      "{'doc': 'Stuve_dev85', 'remove': []}\n",
      "{'doc': 'X-linked-Retinoschisis_dev86', 'remove': []}\n",
      "{'doc': 'Triploidy_dev87', 'remove': []}\n",
      "{'doc': 'Sotos-Syndrome_dev88', 'remove': [[['T8', 0], ['T9', 1]]]}\n",
      "{'doc': 'Werner-Syndrome_dev89', 'remove': [[['T3', 1], ['T4', 0]]]}\n",
      "{'doc': 'Sneddon-Syndrome_dev90', 'remove': [[['T6', 2], ['T7', 1]], [['T8', 0], ['T9', 1]], [['T10', 0], ['T11', 1]], [['T15', 0], ['T16', 1]], [['T17', 0], ['T18', 1]], [['T25', 0], ['T26', 0]]]}\n",
      "{'doc': 'Spastic-Paraplegia-52_dev91', 'remove': [[['T8', 0], ['T9', 1]], [['T22', 0], ['T23', 1]]]}\n",
      "{'doc': 'Wilson-Disease_dev92', 'remove': []}\n",
      "{'doc': 'Zollinger_dev93', 'remove': [[['T3', 0], ['T4', 1]], [['T5', 0], ['T6', 1]]]}\n",
      "{'doc': 'Senior-Lken-Syndrome_dev94', 'remove': [[['T4', 0], ['T5', 1]], [['T6', 0], ['T7', 1]]]}\n",
      "{'doc': 'Tongue-Cancer_dev95', 'remove': []}\n",
      "{'doc': 'Tolosa-Hunt-Syndrome_dev96', 'remove': []}\n",
      "{'doc': 'Trichorhinophalangeal-Syndrome-Type-III_dev97', 'remove': [[['T15', 0], ['T16', 1]]]}\n",
      "{'doc': 'Spinal-Muscular-Atrophy-with-Respiratory-Distress_dev98', 'remove': []}\n",
      "{'doc': 'Short-Bowel-Syndrome_dev99', 'remove': []}\n",
      "{'doc': 'Sialidosis_dev100', 'remove': []}\n",
      "{'doc': 'Warburg-Micro-Syndrome_dev101', 'remove': []}\n",
      "{'doc': 'Wolf_dev102', 'remove': [[['T6', 0], ['T7', 1]], [['T11', 0], ['T12', 1]]]}\n",
      "{'doc': 'Tyrosinemia-Type-1_dev103', 'remove': []}\n",
      "{'doc': 'Sporadic-Porencephaly_dev104', 'remove': []}\n",
      "{'doc': 'XYY-Syndrome_dev105', 'remove': []}\n",
      "{'doc': 'Sepiapterin-Reductase-Deficiency_dev106', 'remove': []}\n",
      "{'doc': 'Vernal-Keratonconjunctivitis_dev107', 'remove': [[['T9', 0], ['T10', 1]]]}\n",
      "{'doc': 'Sirenomelia_dev108', 'remove': []}\n",
      "{'doc': 'Severe-Chronic-Neutropenia_dev109', 'remove': []}\n",
      "{'doc': 'Tietze-Syndrome_dev110', 'remove': []}\n",
      "{'doc': 'Superior-Mesenteric-Artery-Syndrome_dev111', 'remove': []}\n",
      "{'doc': 'Werdnig_dev112', 'remove': []}\n",
      "{'doc': 'Yaws_dev113', 'remove': []}\n",
      "{'doc': 'Weill-Marchesani-Syndrome_dev114', 'remove': [[['T10', 0], ['T11', 1]], [['T13', 0], ['T14', 1]]]}\n",
      "{'doc': 'Waardenburg-Syndrome_dev115', 'remove': []}\n",
      "{'doc': 'Wolman-Disease_dev116', 'remove': []}\n",
      "{'doc': 'Zellweger-Spectrum-Disorders_dev117', 'remove': []}\n",
      "{'doc': 'Yellow-Fever_dev118', 'remove': []}\n",
      "{'doc': 'SHORT-Syndrome_dev119', 'remove': []}\n",
      "{'doc': 'Sennetsu-Fever_dev120', 'remove': []}\n",
      "{'doc': 'Smith-Magenis-Syndrome_dev121', 'remove': []}\n",
      "{'doc': 'Trichotillomania_dev122', 'remove': []}\n",
      "{'doc': 'Segawa-Syndrome_dev123', 'remove': []}\n",
      "{'doc': 'Usher-Syndrome_dev124', 'remove': []}\n",
      "{'doc': 'Thyroid-Cancer_dev125', 'remove': []}\n",
      "{'doc': 'Systemic-Capillary-Leak-Syndrome_dev126', 'remove': [[['T21', 0], ['T22', 1]]]}\n",
      "{'doc': 'Simpson-Dysmorphia-Syndrome_dev127', 'remove': [[['T13', 0], ['T14', 1]], [['T15', 0], ['T16', 1]]]}\n",
      "{'doc': 'Staphylococcal-Scalded-Skin-Syndrome_dev128', 'remove': []}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(res)):\n",
    "    print(res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37642562",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09ab21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21cd3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7b5a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    if not train[doc]['doc_key'] == train_re[doc]['doc'] == cache[doc]['doc']:\n",
    "        print('Document key error happens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60eb2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether all entities are continuous or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "359c79c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    for i in range(len(train[doc]['ner_modified'])):\n",
    "        if len(train[doc]['ner_modified'][i]) != 2:\n",
    "            print('There are still discontinuous entities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00a1058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9772433d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[15, 15], ['SIGN', 'DISEASE']], [[81, 82], ['SYMPTOM', 'SIGN', 'DISEASE']]]\n"
     ]
    }
   ],
   "source": [
    "def find_lists_same_offsets_diff_types(list_of_lists):\n",
    "    # Maintain a dictionary where keys are offsets and values are sets of types\n",
    "    offset_to_types = {}\n",
    "    for lst in list_of_lists:\n",
    "        offset, type = tuple(lst[0]), lst[1]\n",
    "        if offset not in offset_to_types:\n",
    "            offset_to_types[offset] = set()\n",
    "        offset_to_types[offset].add(type)\n",
    "\n",
    "    # Keep only those entries with more than one type\n",
    "    result = [[list(offset), list(types)] for offset, types in offset_to_types.items() if len(types) > 1]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "list_of_lists = [[[0, 1], 'RAREDISEASE'],\n",
    "                 [[5, 6], 'DISEASE'],\n",
    "                 [[15, 15], 'DISEASE'],\n",
    "                 [[15, 15], 'SIGN'],\n",
    "                 [[53, 53], 'SYMPTOM'],\n",
    "                 [[60, 60], 'DISEASE'],\n",
    "                 [[70, 77], 'SIGN'],\n",
    "                 [[79, 79], 'SYMPTOM'],\n",
    "                 [[81, 82], 'SIGN'],\n",
    "                 [[81, 82], 'DISEASE'],\n",
    "                 [[81, 82], 'SYMPTOM'],\n",
    "                 [[96, 97], 'SIGN'],\n",
    "                 [[99, 101], 'SIGN']]\n",
    "\n",
    "print(find_lists_same_offsets_diff_types(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b65c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "6\n",
      "7\n",
      "12\n",
      "13\n",
      "14\n",
      "19\n",
      "21\n",
      "23\n",
      "24\n",
      "25\n",
      "33\n",
      "34\n",
      "39\n",
      "44\n",
      "45\n",
      "47\n",
      "52\n",
      "53\n",
      "56\n",
      "60\n",
      "61\n",
      "74\n",
      "75\n",
      "77\n",
      "78\n",
      "81\n",
      "83\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "93\n",
      "94\n",
      "97\n",
      "102\n",
      "107\n",
      "114\n",
      "126\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "for doc in range(len(train)):\n",
    "    if find_lists_same_offsets_diff_types(train[doc]['ner_modified']) != []:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778ed81",
   "metadata": {},
   "source": [
    "# Remove entities with double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a3a2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists):\n",
    "    \n",
    "    # All indices of discontinuous entities with more than two fragments    \n",
    "    ner_lists_raw_removed = []\n",
    "    for idx in range(len(ner_lists_raw)):\n",
    "        if len(ner_lists_raw[idx]) > 3:\n",
    "            ner_lists_raw_removed.append(idx)\n",
    "\n",
    "    # Create a dictionary to store the names and associated lists\n",
    "    dict_ner_lists_raw = {f\"T{i+1}\": ner_lists_raw[i] for i in range(len(ner_lists_raw))}\n",
    "\n",
    "    # Remove the elements in ner_lists_raw_removed from the dictionary\n",
    "    for idx in ner_lists_raw_removed:\n",
    "        del dict_ner_lists_raw[f\"T{idx+1}\"]\n",
    "\n",
    "    # Convert the dictionary items to a list\n",
    "    items = list(dict_ner_lists_raw.items())\n",
    "\n",
    "    # Create a new dictionary after deleting discontinuous entities with more than two fragments   \n",
    "    new_dict_ner_lists = {}\n",
    "    for k in range(len(ner_lists)):\n",
    "        new_dict_ner_lists[items[k][0]] = ner_lists[k]\n",
    "        \n",
    "    return new_dict_ner_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50c5d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    \n",
    "    train[doc]['relations'] = []\n",
    "    \n",
    "    ner_lists_raw = train[doc]['ner']\n",
    "    ner_lists = train[doc]['ner_modified']\n",
    "        \n",
    "    new_dict_ner_lists = create_dict_deleting_discont_morethan2(ner_lists_raw, ner_lists)\n",
    "    \n",
    "    \n",
    "    # Add relations\n",
    "    if 'relations' in train_re[doc]:\n",
    "\n",
    "        for j in range(len(train_re[doc]['relations'])):\n",
    "\n",
    "            for key, value in train_re[doc]['relations'][j].items():\n",
    "\n",
    "                if value[0] in new_dict_ner_lists and value[1] in new_dict_ner_lists:\n",
    "                    rel_temp = []\n",
    "                    rel_temp += (new_dict_ner_lists[value[0]][0] + new_dict_ner_lists[value[1]][0])\n",
    "                    rel_temp.append(key)\n",
    "\n",
    "            train[doc]['relations'].append(rel_temp)\n",
    "    \n",
    "    \n",
    "    if cache[doc]['remove'] != []: # Need to remove a few entities and relations\n",
    "        \n",
    "        for i in range(len(cache[doc]['remove'])):\n",
    "\n",
    "            a_num = cache[doc]['remove'][i][0][1]\n",
    "            a_name = cache[doc]['remove'][i][0][0]\n",
    "\n",
    "            b_num = cache[doc]['remove'][i][1][1]\n",
    "            b_name = cache[doc]['remove'][i][1][0]\n",
    "\n",
    "            if a_num == 0 and b_num == 0:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                \n",
    "                remove_name = cache[doc]['remove'][i][chosen_element][0]\n",
    "                remove_offsets = new_dict_ner_lists[remove_name]\n",
    "\n",
    "                train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "            elif (a_num == 0 and b_num >= 1) or (b_num == 0 and a_num >= 1):\n",
    "\n",
    "                if a_num == 0:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[a_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                elif b_num == 0:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[b_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "            elif a_num == 1 and b_num == 1:\n",
    "\n",
    "                chosen_element = random.choice([0, 1])\n",
    "                \n",
    "                remove_name = cache[doc]['remove'][i][chosen_element][0]\n",
    "                remove_offsets = new_dict_ner_lists[remove_name]\n",
    "\n",
    "                train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "\n",
    "                result = []\n",
    "                for dictionary in train_re[doc]['relations']:\n",
    "                    for key, values in dictionary.items():\n",
    "                        if remove_name in values:\n",
    "                            result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                for item in result:\n",
    "                    rel_temp = []\n",
    "                    rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                    rel_temp.append(item[2])\n",
    "\n",
    "                    train[doc]['relations'].remove(rel_temp)\n",
    "\n",
    "            else: # (a_num = 1, b_num = N) or (a_num = N, b_num = 1)\n",
    "\n",
    "\n",
    "                if a_num == 1:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[a_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                    result = []\n",
    "                    for dictionary in train_re[doc]['relations']:\n",
    "                        for key, values in dictionary.items():\n",
    "                            if a_name in values:\n",
    "                                result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                    for item in result:\n",
    "                        rel_temp = []\n",
    "                        rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                        rel_temp.append(item[2])\n",
    "\n",
    "                        train[doc]['relations'].remove(rel_temp)\n",
    "\n",
    "                elif b_num == 1:\n",
    "\n",
    "                    remove_offsets = new_dict_ner_lists[b_name]\n",
    "                    train[doc]['ner_modified'].remove(remove_offsets)\n",
    "\n",
    "                    result = []\n",
    "                    for dictionary in train_re[doc]['relations']:\n",
    "                        for key, values in dictionary.items():\n",
    "                            if b_name in values:\n",
    "                                result.append([values[0], values[1], key])\n",
    "\n",
    "\n",
    "                    for item in result:\n",
    "                        rel_temp = []\n",
    "                        rel_temp += (new_dict_ner_lists[item[0]][0] + new_dict_ner_lists[item[1]][0])\n",
    "                        rel_temp.append(item[2])\n",
    "\n",
    "                        train[doc]['relations'].remove(rel_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59dca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check double labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec7ad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(train)):\n",
    "    if find_lists_same_offsets_diff_types(train[doc]['ner_modified']) != []:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6920293",
   "metadata": {},
   "source": [
    "# Addjust the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0382239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i].pop('sentences')\n",
    "    train[i].pop('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e54fde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i]['sentences'] = train[i].pop('sentences_modified')\n",
    "    train[i]['ner'] = train[i].pop('ner_modified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "811d2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flattened_list = []\n",
    "    for sublist in nested_list:\n",
    "        if isinstance(sublist, list):\n",
    "            flattened_list.extend(sublist)\n",
    "        else:\n",
    "            flattened_list.append(sublist)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4d8bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    \n",
    "    train[i]['sentences'] = [train[i]['sentences']]\n",
    "    train[i]['ner'] = [train[i]['ner']]\n",
    "    train[i]['relations'] = [train[i]['relations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70458297",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"/Users/XA/Desktop/Raredis/Pipeline/preprocessed_data/dev/dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7105b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, \"w\") as f_out:\n",
    "    for line in train:\n",
    "        f_out.write(json.dumps(line))\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabab12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1aebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
